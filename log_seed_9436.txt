save path : ./logs/cifar10_resnet18_norm2_rate0.7
{'arch': 'ResNet18', 'batch_size': 128, 'data_path': './data', 'dataset': 'cifar10', 'decay': 0.0005, 'epoch_prune': 1, 'epochs': 200, 'evaluate': False, 'gammas': [10, 0.2, 0.2, 0.2], 'layer_begin': 1, 'layer_end': 57, 'layer_inter': 3, 'learning_rate': 0.01, 'manualSeed': 9436, 'momentum': 0.9, 'ngpu': [0, 1], 'print_freq': 200, 'rate': 0.3, 'resume': '', 'save_path': './logs/cifar10_resnet18_norm2_rate0.7', 'schedule': [1, 60, 120, 160], 'start_epoch': 0, 'use_cuda': True, 'use_state_dict': False, 'workers': 4}
Random Seed: 9436
python version : 3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56)  [GCC 7.2.0]
torch  version : 0.4.1.post2
cudnn  version : 7102
Compress Rate: 0.3
Layer Begin: 1
Layer End: 57
Layer Inter: 3
Epoch prune: 1
=> creating model 'ResNet18'
=> network :
 ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=512, out_features=10, bias=True)
)
=> do not use any checkpoint for ResNet18 model
  **Test** Prec@1 10.010 Prec@5 46.640 Error@1 89.990
  **Test** Prec@1 10.010 Prec@5 46.640 Error@1 89.990

==>>[2018-11-14 10:59:37] [Epoch=000/200] [Need: 00:00:00] [learning_rate=0.0100] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 0.787 (0.787)   Data 0.321 (0.321)   Loss 2.3204 (2.3204)   Prec@1 10.938 (10.938)   Prec@5 53.125 (53.125)   [2018-11-14 10:59:38]
  Epoch: [000][200/391]   Time 0.041 (0.048)   Data 0.000 (0.002)   Loss 1.4460 (1.6929)   Prec@1 46.094 (36.796)   Prec@5 93.750 (86.011)   [2018-11-14 10:59:47]
  **Train** Prec@1 44.908 Prec@5 90.094 Error@1 55.092
  **Test** Prec@1 57.160 Prec@5 95.630 Error@1 42.840
  **Test** Prec@1 57.160 Prec@5 95.630 Error@1 42.840

==>>[2018-11-14 11:00:00] [Epoch=001/200] [Need: 01:15:13] [learning_rate=0.1000] [Best : Accuracy=57.16, Error=42.84]
  Epoch: [001][000/391]   Time 0.302 (0.302)   Data 0.244 (0.244)   Loss 1.0772 (1.0772)   Prec@1 63.281 (63.281)   Prec@5 96.875 (96.875)   [2018-11-14 11:00:00]
  Epoch: [001][200/391]   Time 0.044 (0.046)   Data 0.000 (0.001)   Loss 1.0660 (1.3233)   Prec@1 56.250 (52.200)   Prec@5 96.094 (93.256)   [2018-11-14 11:00:09]
  **Train** Prec@1 60.114 Prec@5 95.244 Error@1 39.886
  **Test** Prec@1 61.630 Prec@5 95.090 Error@1 38.370
  **Test** Prec@1 61.630 Prec@5 95.090 Error@1 38.370

==>>[2018-11-14 11:00:22] [Epoch=002/200] [Need: 01:13:14] [learning_rate=0.1000] [Best : Accuracy=61.63, Error=38.37]
  Epoch: [002][000/391]   Time 0.325 (0.325)   Data 0.274 (0.274)   Loss 0.8501 (0.8501)   Prec@1 72.656 (72.656)   Prec@5 97.656 (97.656)   [2018-11-14 11:00:22]
  Epoch: [002][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.7643 (0.7205)   Prec@1 71.094 (74.674)   Prec@5 99.219 (98.515)   [2018-11-14 11:00:31]
  **Train** Prec@1 75.772 Prec@5 98.604 Error@1 24.228
  **Test** Prec@1 73.450 Prec@5 98.290 Error@1 26.550
  **Test** Prec@1 73.450 Prec@5 98.290 Error@1 26.550

==>>[2018-11-14 11:00:44] [Epoch=003/200] [Need: 01:12:42] [learning_rate=0.1000] [Best : Accuracy=73.45, Error=26.55]
  Epoch: [003][000/391]   Time 0.311 (0.311)   Data 0.259 (0.259)   Loss 0.6500 (0.6500)   Prec@1 78.906 (78.906)   Prec@5 97.656 (97.656)   [2018-11-14 11:00:44]
  Epoch: [003][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.5984 (0.5669)   Prec@1 78.125 (80.628)   Prec@5 100.000 (98.935)   [2018-11-14 11:00:53]
  **Train** Prec@1 80.934 Prec@5 98.976 Error@1 19.066
  **Test** Prec@1 79.200 Prec@5 98.910 Error@1 20.800
  **Test** Prec@1 79.200 Prec@5 98.910 Error@1 20.800

==>>[2018-11-14 11:01:06] [Epoch=004/200] [Need: 01:12:15] [learning_rate=0.1000] [Best : Accuracy=79.20, Error=20.80]
  Epoch: [004][000/391]   Time 0.331 (0.331)   Data 0.258 (0.258)   Loss 0.5174 (0.5174)   Prec@1 84.375 (84.375)   Prec@5 99.219 (99.219)   [2018-11-14 11:01:06]
  Epoch: [004][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.3667 (0.4794)   Prec@1 87.500 (83.617)   Prec@5 100.000 (99.265)   [2018-11-14 11:01:15]
  **Train** Prec@1 83.364 Prec@5 99.206 Error@1 16.636
  **Test** Prec@1 80.340 Prec@5 99.180 Error@1 19.660
  **Test** Prec@1 80.340 Prec@5 99.180 Error@1 19.660

==>>[2018-11-14 11:01:28] [Epoch=005/200] [Need: 01:11:43] [learning_rate=0.1000] [Best : Accuracy=80.34, Error=19.66]
  Epoch: [005][000/391]   Time 0.296 (0.296)   Data 0.245 (0.245)   Loss 0.4388 (0.4388)   Prec@1 85.156 (85.156)   Prec@5 99.219 (99.219)   [2018-11-14 11:01:28]
  Epoch: [005][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.4395 (0.4411)   Prec@1 87.500 (84.880)   Prec@5 98.438 (99.374)   [2018-11-14 11:01:37]
  **Train** Prec@1 84.744 Prec@5 99.396 Error@1 15.256
  **Test** Prec@1 78.980 Prec@5 98.560 Error@1 21.020
  **Test** Prec@1 78.980 Prec@5 98.560 Error@1 21.020

==>>[2018-11-14 11:01:49] [Epoch=006/200] [Need: 01:11:13] [learning_rate=0.1000] [Best : Accuracy=80.34, Error=19.66]
  Epoch: [006][000/391]   Time 0.334 (0.334)   Data 0.277 (0.277)   Loss 0.4716 (0.4716)   Prec@1 82.031 (82.031)   Prec@5 100.000 (100.000)   [2018-11-14 11:01:50]
  Epoch: [006][200/391]   Time 0.048 (0.047)   Data 0.000 (0.002)   Loss 0.4175 (0.4077)   Prec@1 85.156 (85.844)   Prec@5 98.438 (99.545)   [2018-11-14 11:01:59]
  **Train** Prec@1 85.636 Prec@5 99.470 Error@1 14.364
  **Test** Prec@1 79.650 Prec@5 98.610 Error@1 20.350
  **Test** Prec@1 79.650 Prec@5 98.610 Error@1 20.350

==>>[2018-11-14 11:02:11] [Epoch=007/200] [Need: 01:10:48] [learning_rate=0.1000] [Best : Accuracy=80.34, Error=19.66]
  Epoch: [007][000/391]   Time 0.317 (0.317)   Data 0.261 (0.261)   Loss 0.4027 (0.4027)   Prec@1 90.625 (90.625)   Prec@5 99.219 (99.219)   [2018-11-14 11:02:12]
  Epoch: [007][200/391]   Time 0.045 (0.047)   Data 0.000 (0.002)   Loss 0.3962 (0.3770)   Prec@1 86.719 (86.855)   Prec@5 100.000 (99.592)   [2018-11-14 11:02:21]
  **Train** Prec@1 86.506 Prec@5 99.560 Error@1 13.494
  **Test** Prec@1 82.020 Prec@5 98.970 Error@1 17.980
  **Test** Prec@1 82.020 Prec@5 98.970 Error@1 17.980

==>>[2018-11-14 11:02:33] [Epoch=008/200] [Need: 01:10:27] [learning_rate=0.1000] [Best : Accuracy=82.02, Error=17.98]
  Epoch: [008][000/391]   Time 0.302 (0.302)   Data 0.245 (0.245)   Loss 0.3268 (0.3268)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2018-11-14 11:02:34]
  Epoch: [008][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.4574 (0.3457)   Prec@1 85.938 (87.974)   Prec@5 100.000 (99.670)   [2018-11-14 11:02:43]
  **Train** Prec@1 87.492 Prec@5 99.604 Error@1 12.508
  **Test** Prec@1 80.800 Prec@5 99.010 Error@1 19.200
  **Test** Prec@1 80.800 Prec@5 99.010 Error@1 19.200

==>>[2018-11-14 11:02:55] [Epoch=009/200] [Need: 01:09:58] [learning_rate=0.1000] [Best : Accuracy=82.02, Error=17.98]
  Epoch: [009][000/391]   Time 0.296 (0.296)   Data 0.248 (0.248)   Loss 0.3934 (0.3934)   Prec@1 88.281 (88.281)   Prec@5 99.219 (99.219)   [2018-11-14 11:02:55]
  Epoch: [009][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.4563 (0.3344)   Prec@1 84.375 (88.604)   Prec@5 100.000 (99.666)   [2018-11-14 11:03:04]
  **Train** Prec@1 88.098 Prec@5 99.612 Error@1 11.902
  **Test** Prec@1 83.750 Prec@5 99.250 Error@1 16.250
  **Test** Prec@1 83.750 Prec@5 99.250 Error@1 16.250

==>>[2018-11-14 11:03:17] [Epoch=010/200] [Need: 01:09:36] [learning_rate=0.1000] [Best : Accuracy=83.75, Error=16.25]
  Epoch: [010][000/391]   Time 0.322 (0.322)   Data 0.254 (0.254)   Loss 0.3292 (0.3292)   Prec@1 89.844 (89.844)   Prec@5 98.438 (98.438)   [2018-11-14 11:03:17]
  Epoch: [010][200/391]   Time 0.044 (0.045)   Data 0.000 (0.002)   Loss 0.3067 (0.3184)   Prec@1 89.844 (89.311)   Prec@5 100.000 (99.689)   [2018-11-14 11:03:26]
  **Train** Prec@1 88.758 Prec@5 99.606 Error@1 11.242
  **Test** Prec@1 84.260 Prec@5 99.320 Error@1 15.740
  **Test** Prec@1 84.260 Prec@5 99.320 Error@1 15.740

==>>[2018-11-14 11:03:39] [Epoch=011/200] [Need: 01:09:11] [learning_rate=0.1000] [Best : Accuracy=84.26, Error=15.74]
  Epoch: [011][000/391]   Time 0.323 (0.323)   Data 0.270 (0.270)   Loss 0.1576 (0.1576)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-11-14 11:03:39]
  Epoch: [011][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.2733 (0.3009)   Prec@1 89.844 (89.684)   Prec@5 100.000 (99.681)   [2018-11-14 11:03:48]
  **Train** Prec@1 89.012 Prec@5 99.616 Error@1 10.988
  **Test** Prec@1 83.690 Prec@5 99.240 Error@1 16.310
  **Test** Prec@1 83.690 Prec@5 99.240 Error@1 16.310

==>>[2018-11-14 11:04:01] [Epoch=012/200] [Need: 01:08:45] [learning_rate=0.1000] [Best : Accuracy=84.26, Error=15.74]
  Epoch: [012][000/391]   Time 0.317 (0.317)   Data 0.257 (0.257)   Loss 0.1957 (0.1957)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2018-11-14 11:04:01]
  Epoch: [012][200/391]   Time 0.050 (0.046)   Data 0.000 (0.002)   Loss 0.3434 (0.3048)   Prec@1 89.844 (89.506)   Prec@5 99.219 (99.705)   [2018-11-14 11:04:10]
  **Train** Prec@1 89.118 Prec@5 99.718 Error@1 10.882
  **Test** Prec@1 83.420 Prec@5 99.200 Error@1 16.580
  **Test** Prec@1 83.420 Prec@5 99.200 Error@1 16.580

==>>[2018-11-14 11:04:22] [Epoch=013/200] [Need: 01:08:19] [learning_rate=0.1000] [Best : Accuracy=84.26, Error=15.74]
  Epoch: [013][000/391]   Time 0.319 (0.319)   Data 0.264 (0.264)   Loss 0.3554 (0.3554)   Prec@1 86.719 (86.719)   Prec@5 100.000 (100.000)   [2018-11-14 11:04:23]
  Epoch: [013][200/391]   Time 0.048 (0.047)   Data 0.000 (0.002)   Loss 0.3454 (0.2871)   Prec@1 86.719 (89.972)   Prec@5 100.000 (99.724)   [2018-11-14 11:04:32]
  **Train** Prec@1 89.574 Prec@5 99.722 Error@1 10.426
  **Test** Prec@1 82.650 Prec@5 98.450 Error@1 17.350
  **Test** Prec@1 82.650 Prec@5 98.450 Error@1 17.350

==>>[2018-11-14 11:04:44] [Epoch=014/200] [Need: 01:07:56] [learning_rate=0.1000] [Best : Accuracy=84.26, Error=15.74]
  Epoch: [014][000/391]   Time 0.304 (0.304)   Data 0.252 (0.252)   Loss 0.2671 (0.2671)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2018-11-14 11:04:44]
  Epoch: [014][200/391]   Time 0.040 (0.045)   Data 0.000 (0.002)   Loss 0.3474 (0.2778)   Prec@1 87.500 (90.450)   Prec@5 100.000 (99.732)   [2018-11-14 11:04:53]
  **Train** Prec@1 89.990 Prec@5 99.742 Error@1 10.010
  **Test** Prec@1 80.200 Prec@5 98.990 Error@1 19.800
  **Test** Prec@1 80.200 Prec@5 98.990 Error@1 19.800

==>>[2018-11-14 11:05:05] [Epoch=015/200] [Need: 01:07:25] [learning_rate=0.1000] [Best : Accuracy=84.26, Error=15.74]
  Epoch: [015][000/391]   Time 0.310 (0.310)   Data 0.254 (0.254)   Loss 0.3027 (0.3027)   Prec@1 90.625 (90.625)   Prec@5 100.000 (100.000)   [2018-11-14 11:05:06]
  Epoch: [015][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.3440 (0.2647)   Prec@1 89.062 (90.932)   Prec@5 100.000 (99.790)   [2018-11-14 11:05:15]
  **Train** Prec@1 90.212 Prec@5 99.748 Error@1 9.788
  **Test** Prec@1 79.530 Prec@5 98.830 Error@1 20.470
  **Test** Prec@1 79.530 Prec@5 98.830 Error@1 20.470

==>>[2018-11-14 11:05:27] [Epoch=016/200] [Need: 01:07:02] [learning_rate=0.1000] [Best : Accuracy=84.26, Error=15.74]
  Epoch: [016][000/391]   Time 0.295 (0.295)   Data 0.239 (0.239)   Loss 0.1665 (0.1665)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:05:27]
  Epoch: [016][200/391]   Time 0.044 (0.046)   Data 0.000 (0.001)   Loss 0.2462 (0.2566)   Prec@1 92.188 (91.367)   Prec@5 100.000 (99.810)   [2018-11-14 11:05:36]
  **Train** Prec@1 90.562 Prec@5 99.782 Error@1 9.438
  **Test** Prec@1 84.570 Prec@5 99.330 Error@1 15.430
  **Test** Prec@1 84.570 Prec@5 99.330 Error@1 15.430

==>>[2018-11-14 11:05:49] [Epoch=017/200] [Need: 01:06:42] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [017][000/391]   Time 0.306 (0.306)   Data 0.249 (0.249)   Loss 0.2079 (0.2079)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2018-11-14 11:05:49]
  Epoch: [017][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.3621 (0.2603)   Prec@1 85.938 (91.037)   Prec@5 100.000 (99.767)   [2018-11-14 11:05:58]
  **Train** Prec@1 90.324 Prec@5 99.748 Error@1 9.676
  **Test** Prec@1 82.760 Prec@5 99.060 Error@1 17.240
  **Test** Prec@1 82.760 Prec@5 99.060 Error@1 17.240

==>>[2018-11-14 11:06:11] [Epoch=018/200] [Need: 01:06:19] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [018][000/391]   Time 0.317 (0.317)   Data 0.256 (0.256)   Loss 0.1991 (0.1991)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-11-14 11:06:11]
  Epoch: [018][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.3812 (0.2494)   Prec@1 89.844 (91.426)   Prec@5 100.000 (99.829)   [2018-11-14 11:06:20]
  **Train** Prec@1 90.668 Prec@5 99.760 Error@1 9.332
  **Test** Prec@1 83.000 Prec@5 99.110 Error@1 17.000
  **Test** Prec@1 83.000 Prec@5 99.110 Error@1 17.000

==>>[2018-11-14 11:06:33] [Epoch=019/200] [Need: 01:05:56] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [019][000/391]   Time 0.321 (0.321)   Data 0.266 (0.266)   Loss 0.1789 (0.1789)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:06:33]
  Epoch: [019][200/391]   Time 0.050 (0.047)   Data 0.000 (0.002)   Loss 0.1660 (0.2524)   Prec@1 93.750 (91.165)   Prec@5 100.000 (99.856)   [2018-11-14 11:06:42]
  **Train** Prec@1 90.836 Prec@5 99.822 Error@1 9.164
  **Test** Prec@1 80.970 Prec@5 98.950 Error@1 19.030
  **Test** Prec@1 80.970 Prec@5 98.950 Error@1 19.030

==>>[2018-11-14 11:06:55] [Epoch=020/200] [Need: 01:05:36] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [020][000/391]   Time 0.301 (0.301)   Data 0.244 (0.244)   Loss 0.2358 (0.2358)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-11-14 11:06:55]
  Epoch: [020][200/391]   Time 0.045 (0.047)   Data 0.000 (0.001)   Loss 0.2338 (0.2508)   Prec@1 91.406 (91.045)   Prec@5 100.000 (99.848)   [2018-11-14 11:07:04]
  **Train** Prec@1 90.628 Prec@5 99.802 Error@1 9.372
  **Test** Prec@1 84.040 Prec@5 99.270 Error@1 15.960
  **Test** Prec@1 84.040 Prec@5 99.270 Error@1 15.960

==>>[2018-11-14 11:07:17] [Epoch=021/200] [Need: 01:05:14] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [021][000/391]   Time 0.316 (0.316)   Data 0.263 (0.263)   Loss 0.2138 (0.2138)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2018-11-14 11:07:17]
  Epoch: [021][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.2076 (0.2424)   Prec@1 92.969 (91.686)   Prec@5 100.000 (99.829)   [2018-11-14 11:07:26]
  **Train** Prec@1 91.200 Prec@5 99.810 Error@1 8.800
  **Test** Prec@1 80.410 Prec@5 98.900 Error@1 19.590
  **Test** Prec@1 80.410 Prec@5 98.900 Error@1 19.590

==>>[2018-11-14 11:07:38] [Epoch=022/200] [Need: 01:04:51] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [022][000/391]   Time 0.313 (0.313)   Data 0.266 (0.266)   Loss 0.2343 (0.2343)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2018-11-14 11:07:39]
  Epoch: [022][200/391]   Time 0.044 (0.047)   Data 0.000 (0.002)   Loss 0.2563 (0.2340)   Prec@1 87.500 (92.001)   Prec@5 100.000 (99.802)   [2018-11-14 11:07:48]
  **Train** Prec@1 91.348 Prec@5 99.794 Error@1 8.652
  **Test** Prec@1 84.050 Prec@5 99.170 Error@1 15.950
  **Test** Prec@1 84.050 Prec@5 99.170 Error@1 15.950

==>>[2018-11-14 11:08:00] [Epoch=023/200] [Need: 01:04:30] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [023][000/391]   Time 0.303 (0.303)   Data 0.244 (0.244)   Loss 0.1701 (0.1701)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:08:01]
  Epoch: [023][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.3300 (0.2320)   Prec@1 87.500 (91.985)   Prec@5 100.000 (99.837)   [2018-11-14 11:08:09]
  **Train** Prec@1 91.426 Prec@5 99.820 Error@1 8.574
  **Test** Prec@1 80.950 Prec@5 99.030 Error@1 19.050
  **Test** Prec@1 80.950 Prec@5 99.030 Error@1 19.050

==>>[2018-11-14 11:08:22] [Epoch=024/200] [Need: 01:04:07] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [024][000/391]   Time 0.295 (0.295)   Data 0.244 (0.244)   Loss 0.1372 (0.1372)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:08:22]
  Epoch: [024][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.2726 (0.2305)   Prec@1 89.062 (92.129)   Prec@5 100.000 (99.810)   [2018-11-14 11:08:31]
  **Train** Prec@1 91.384 Prec@5 99.818 Error@1 8.616
  **Test** Prec@1 82.970 Prec@5 99.310 Error@1 17.030
  **Test** Prec@1 82.970 Prec@5 99.310 Error@1 17.030

==>>[2018-11-14 11:08:44] [Epoch=025/200] [Need: 01:03:44] [learning_rate=0.1000] [Best : Accuracy=84.57, Error=15.43]
  Epoch: [025][000/391]   Time 0.301 (0.301)   Data 0.244 (0.244)   Loss 0.1496 (0.1496)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:08:44]
  Epoch: [025][200/391]   Time 0.045 (0.046)   Data 0.000 (0.001)   Loss 0.2382 (0.2291)   Prec@1 92.969 (92.234)   Prec@5 98.438 (99.802)   [2018-11-14 11:08:53]
  **Train** Prec@1 91.634 Prec@5 99.782 Error@1 8.366
  **Test** Prec@1 84.980 Prec@5 99.310 Error@1 15.020
  **Test** Prec@1 84.980 Prec@5 99.310 Error@1 15.020

==>>[2018-11-14 11:09:05] [Epoch=026/200] [Need: 01:03:22] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [026][000/391]   Time 0.314 (0.314)   Data 0.258 (0.258)   Loss 0.1767 (0.1767)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:09:06]
  Epoch: [026][200/391]   Time 0.043 (0.045)   Data 0.000 (0.002)   Loss 0.2741 (0.2276)   Prec@1 89.062 (92.269)   Prec@5 100.000 (99.848)   [2018-11-14 11:09:15]
  **Train** Prec@1 91.750 Prec@5 99.836 Error@1 8.250
  **Test** Prec@1 82.210 Prec@5 99.320 Error@1 17.790
  **Test** Prec@1 82.210 Prec@5 99.320 Error@1 17.790

==>>[2018-11-14 11:09:27] [Epoch=027/200] [Need: 01:02:58] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [027][000/391]   Time 0.306 (0.306)   Data 0.250 (0.250)   Loss 0.2065 (0.2065)   Prec@1 93.750 (93.750)   Prec@5 99.219 (99.219)   [2018-11-14 11:09:27]
  Epoch: [027][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.2319 (0.2180)   Prec@1 92.188 (92.631)   Prec@5 100.000 (99.880)   [2018-11-14 11:09:36]
  **Train** Prec@1 91.956 Prec@5 99.848 Error@1 8.044
  **Test** Prec@1 84.720 Prec@5 99.370 Error@1 15.280
  **Test** Prec@1 84.720 Prec@5 99.370 Error@1 15.280

==>>[2018-11-14 11:09:49] [Epoch=028/200] [Need: 01:02:35] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [028][000/391]   Time 0.295 (0.295)   Data 0.246 (0.246)   Loss 0.1537 (0.1537)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-11-14 11:09:49]
  Epoch: [028][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.3297 (0.2243)   Prec@1 90.625 (92.238)   Prec@5 100.000 (99.848)   [2018-11-14 11:09:58]
  **Train** Prec@1 91.524 Prec@5 99.806 Error@1 8.476
  **Test** Prec@1 83.130 Prec@5 98.490 Error@1 16.870
  **Test** Prec@1 83.130 Prec@5 98.490 Error@1 16.870

==>>[2018-11-14 11:10:10] [Epoch=029/200] [Need: 01:02:12] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [029][000/391]   Time 0.302 (0.302)   Data 0.246 (0.246)   Loss 0.2608 (0.2608)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2018-11-14 11:10:11]
  Epoch: [029][200/391]   Time 0.046 (0.046)   Data 0.000 (0.001)   Loss 0.1588 (0.2224)   Prec@1 95.312 (92.394)   Prec@5 100.000 (99.856)   [2018-11-14 11:10:19]
  **Train** Prec@1 91.784 Prec@5 99.838 Error@1 8.216
  **Test** Prec@1 80.250 Prec@5 98.290 Error@1 19.750
  **Test** Prec@1 80.250 Prec@5 98.290 Error@1 19.750

==>>[2018-11-14 11:10:32] [Epoch=030/200] [Need: 01:01:50] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [030][000/391]   Time 0.301 (0.301)   Data 0.244 (0.244)   Loss 0.2226 (0.2226)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-11-14 11:10:32]
  Epoch: [030][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.2337 (0.2177)   Prec@1 92.188 (92.417)   Prec@5 100.000 (99.876)   [2018-11-14 11:10:41]
  **Train** Prec@1 91.654 Prec@5 99.820 Error@1 8.346
  **Test** Prec@1 83.460 Prec@5 99.140 Error@1 16.540
  **Test** Prec@1 83.460 Prec@5 99.140 Error@1 16.540

==>>[2018-11-14 11:10:54] [Epoch=031/200] [Need: 01:01:28] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [031][000/391]   Time 0.307 (0.307)   Data 0.250 (0.250)   Loss 0.1522 (0.1522)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:10:54]
  Epoch: [031][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.1945 (0.2161)   Prec@1 95.312 (92.615)   Prec@5 100.000 (99.880)   [2018-11-14 11:11:03]
  **Train** Prec@1 91.878 Prec@5 99.834 Error@1 8.122
  **Test** Prec@1 83.950 Prec@5 99.030 Error@1 16.050
  **Test** Prec@1 83.950 Prec@5 99.030 Error@1 16.050

==>>[2018-11-14 11:11:16] [Epoch=032/200] [Need: 01:01:06] [learning_rate=0.1000] [Best : Accuracy=84.98, Error=15.02]
  Epoch: [032][000/391]   Time 0.324 (0.324)   Data 0.271 (0.271)   Loss 0.2701 (0.2701)   Prec@1 89.062 (89.062)   Prec@5 100.000 (100.000)   [2018-11-14 11:11:16]
  Epoch: [032][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.2006 (0.2245)   Prec@1 92.188 (92.335)   Prec@5 100.000 (99.821)   [2018-11-14 11:11:25]
  **Train** Prec@1 91.968 Prec@5 99.822 Error@1 8.032
  **Test** Prec@1 85.040 Prec@5 99.180 Error@1 14.960
  **Test** Prec@1 85.040 Prec@5 99.180 Error@1 14.960

==>>[2018-11-14 11:11:38] [Epoch=033/200] [Need: 01:00:45] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [033][000/391]   Time 0.315 (0.315)   Data 0.259 (0.259)   Loss 0.2505 (0.2505)   Prec@1 91.406 (91.406)   Prec@5 100.000 (100.000)   [2018-11-14 11:11:38]
  Epoch: [033][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.2964 (0.2191)   Prec@1 87.500 (92.627)   Prec@5 100.000 (99.856)   [2018-11-14 11:11:47]
  **Train** Prec@1 92.066 Prec@5 99.838 Error@1 7.934
  **Test** Prec@1 82.450 Prec@5 98.960 Error@1 17.550
  **Test** Prec@1 82.450 Prec@5 98.960 Error@1 17.550

==>>[2018-11-14 11:11:59] [Epoch=034/200] [Need: 01:00:22] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [034][000/391]   Time 0.310 (0.310)   Data 0.254 (0.254)   Loss 0.2036 (0.2036)   Prec@1 92.969 (92.969)   Prec@5 100.000 (100.000)   [2018-11-14 11:12:00]
  Epoch: [034][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.2233 (0.2201)   Prec@1 92.969 (92.498)   Prec@5 100.000 (99.864)   [2018-11-14 11:12:09]
  **Train** Prec@1 92.036 Prec@5 99.836 Error@1 7.964
  **Test** Prec@1 83.840 Prec@5 99.280 Error@1 16.160
  **Test** Prec@1 83.840 Prec@5 99.280 Error@1 16.160

==>>[2018-11-14 11:12:21] [Epoch=035/200] [Need: 01:00:00] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [035][000/391]   Time 0.317 (0.317)   Data 0.261 (0.261)   Loss 0.1488 (0.1488)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:12:21]
  Epoch: [035][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.2881 (0.2170)   Prec@1 86.719 (92.724)   Prec@5 100.000 (99.891)   [2018-11-14 11:12:30]
  **Train** Prec@1 92.136 Prec@5 99.856 Error@1 7.864
  **Test** Prec@1 82.810 Prec@5 99.060 Error@1 17.190
  **Test** Prec@1 82.810 Prec@5 99.060 Error@1 17.190

==>>[2018-11-14 11:12:43] [Epoch=036/200] [Need: 00:59:38] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [036][000/391]   Time 0.316 (0.316)   Data 0.261 (0.261)   Loss 0.2215 (0.2215)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-11-14 11:12:43]
  Epoch: [036][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.2596 (0.2225)   Prec@1 92.969 (92.374)   Prec@5 100.000 (99.860)   [2018-11-14 11:12:52]
  **Train** Prec@1 91.984 Prec@5 99.840 Error@1 8.016
  **Test** Prec@1 80.750 Prec@5 99.190 Error@1 19.250
  **Test** Prec@1 80.750 Prec@5 99.190 Error@1 19.250

==>>[2018-11-14 11:13:04] [Epoch=037/200] [Need: 00:59:15] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [037][000/391]   Time 0.291 (0.291)   Data 0.239 (0.239)   Loss 0.1854 (0.1854)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:13:05]
  Epoch: [037][200/391]   Time 0.041 (0.045)   Data 0.000 (0.001)   Loss 0.3418 (0.2161)   Prec@1 85.156 (92.697)   Prec@5 100.000 (99.876)   [2018-11-14 11:13:13]
  **Train** Prec@1 92.140 Prec@5 99.866 Error@1 7.860
  **Test** Prec@1 84.670 Prec@5 99.390 Error@1 15.330
  **Test** Prec@1 84.670 Prec@5 99.390 Error@1 15.330

==>>[2018-11-14 11:13:26] [Epoch=038/200] [Need: 00:58:52] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [038][000/391]   Time 0.297 (0.297)   Data 0.240 (0.240)   Loss 0.1467 (0.1467)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:13:26]
  Epoch: [038][200/391]   Time 0.041 (0.045)   Data 0.000 (0.001)   Loss 0.2487 (0.2119)   Prec@1 89.844 (92.654)   Prec@5 100.000 (99.887)   [2018-11-14 11:13:35]
  **Train** Prec@1 92.148 Prec@5 99.846 Error@1 7.852
  **Test** Prec@1 84.480 Prec@5 99.330 Error@1 15.520
  **Test** Prec@1 84.480 Prec@5 99.330 Error@1 15.520

==>>[2018-11-14 11:13:47] [Epoch=039/200] [Need: 00:58:28] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [039][000/391]   Time 0.296 (0.296)   Data 0.241 (0.241)   Loss 0.1557 (0.1557)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:13:48]
  Epoch: [039][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.1995 (0.2060)   Prec@1 94.531 (92.973)   Prec@5 100.000 (99.887)   [2018-11-14 11:13:56]
  **Train** Prec@1 92.264 Prec@5 99.850 Error@1 7.736
  **Test** Prec@1 82.070 Prec@5 98.870 Error@1 17.930
  **Test** Prec@1 82.070 Prec@5 98.870 Error@1 17.930

==>>[2018-11-14 11:14:09] [Epoch=040/200] [Need: 00:58:06] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [040][000/391]   Time 0.348 (0.348)   Data 0.289 (0.289)   Loss 0.1912 (0.1912)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:14:09]
  Epoch: [040][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.1592 (0.2066)   Prec@1 95.312 (92.942)   Prec@5 100.000 (99.872)   [2018-11-14 11:14:18]
  **Train** Prec@1 92.284 Prec@5 99.854 Error@1 7.716
  **Test** Prec@1 84.410 Prec@5 99.370 Error@1 15.590
  **Test** Prec@1 84.410 Prec@5 99.370 Error@1 15.590

==>>[2018-11-14 11:14:31] [Epoch=041/200] [Need: 00:57:45] [learning_rate=0.1000] [Best : Accuracy=85.04, Error=14.96]
  Epoch: [041][000/391]   Time 0.303 (0.303)   Data 0.248 (0.248)   Loss 0.2380 (0.2380)   Prec@1 92.969 (92.969)   Prec@5 99.219 (99.219)   [2018-11-14 11:14:31]
  Epoch: [041][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.2151 (0.2113)   Prec@1 92.969 (92.786)   Prec@5 100.000 (99.848)   [2018-11-14 11:14:40]
  **Train** Prec@1 92.270 Prec@5 99.828 Error@1 7.730
  **Test** Prec@1 85.270 Prec@5 99.230 Error@1 14.730
  **Test** Prec@1 85.270 Prec@5 99.230 Error@1 14.730

==>>[2018-11-14 11:14:53] [Epoch=042/200] [Need: 00:57:24] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [042][000/391]   Time 0.323 (0.323)   Data 0.259 (0.259)   Loss 0.1605 (0.1605)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:14:53]
  Epoch: [042][200/391]   Time 0.040 (0.045)   Data 0.000 (0.002)   Loss 0.2162 (0.2119)   Prec@1 92.188 (92.732)   Prec@5 99.219 (99.880)   [2018-11-14 11:15:02]
  **Train** Prec@1 92.224 Prec@5 99.862 Error@1 7.776
  **Test** Prec@1 82.160 Prec@5 99.020 Error@1 17.840
  **Test** Prec@1 82.160 Prec@5 99.020 Error@1 17.840

==>>[2018-11-14 11:15:14] [Epoch=043/200] [Need: 00:57:00] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [043][000/391]   Time 0.312 (0.312)   Data 0.255 (0.255)   Loss 0.1644 (0.1644)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:15:14]
  Epoch: [043][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.3060 (0.2106)   Prec@1 89.062 (92.969)   Prec@5 100.000 (99.911)   [2018-11-14 11:15:23]
  **Train** Prec@1 92.356 Prec@5 99.876 Error@1 7.644
  **Test** Prec@1 83.910 Prec@5 99.100 Error@1 16.090
  **Test** Prec@1 83.910 Prec@5 99.100 Error@1 16.090

==>>[2018-11-14 11:15:36] [Epoch=044/200] [Need: 00:56:38] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [044][000/391]   Time 0.315 (0.315)   Data 0.261 (0.261)   Loss 0.2020 (0.2020)   Prec@1 94.531 (94.531)   Prec@5 99.219 (99.219)   [2018-11-14 11:15:36]
  Epoch: [044][200/391]   Time 0.053 (0.045)   Data 0.000 (0.002)   Loss 0.2392 (0.2168)   Prec@1 90.625 (92.662)   Prec@5 99.219 (99.856)   [2018-11-14 11:15:45]
  **Train** Prec@1 92.290 Prec@5 99.872 Error@1 7.710
  **Test** Prec@1 81.590 Prec@5 98.880 Error@1 18.410
  **Test** Prec@1 81.590 Prec@5 98.880 Error@1 18.410

==>>[2018-11-14 11:15:57] [Epoch=045/200] [Need: 00:56:14] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [045][000/391]   Time 0.321 (0.321)   Data 0.266 (0.266)   Loss 0.1669 (0.1669)   Prec@1 94.531 (94.531)   Prec@5 100.000 (100.000)   [2018-11-14 11:15:57]
  Epoch: [045][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.1725 (0.2096)   Prec@1 92.969 (92.852)   Prec@5 100.000 (99.872)   [2018-11-14 11:16:06]
  **Train** Prec@1 92.388 Prec@5 99.842 Error@1 7.612
  **Test** Prec@1 84.240 Prec@5 99.200 Error@1 15.760
  **Test** Prec@1 84.240 Prec@5 99.200 Error@1 15.760

==>>[2018-11-14 11:16:19] [Epoch=046/200] [Need: 00:55:52] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [046][000/391]   Time 0.307 (0.307)   Data 0.258 (0.258)   Loss 0.1832 (0.1832)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-11-14 11:16:19]
  Epoch: [046][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.1776 (0.2106)   Prec@1 95.312 (92.693)   Prec@5 99.219 (99.864)   [2018-11-14 11:16:28]
  **Train** Prec@1 92.190 Prec@5 99.854 Error@1 7.810
  **Test** Prec@1 84.520 Prec@5 99.190 Error@1 15.480
  **Test** Prec@1 84.520 Prec@5 99.190 Error@1 15.480

==>>[2018-11-14 11:16:41] [Epoch=047/200] [Need: 00:55:31] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [047][000/391]   Time 0.319 (0.319)   Data 0.259 (0.259)   Loss 0.1259 (0.1259)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:16:41]
  Epoch: [047][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.2376 (0.2047)   Prec@1 93.750 (93.190)   Prec@5 100.000 (99.891)   [2018-11-14 11:16:50]
  **Train** Prec@1 92.718 Prec@5 99.848 Error@1 7.282
  **Test** Prec@1 79.020 Prec@5 98.750 Error@1 20.980
  **Test** Prec@1 79.020 Prec@5 98.750 Error@1 20.980

==>>[2018-11-14 11:17:02] [Epoch=048/200] [Need: 00:55:08] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [048][000/391]   Time 0.301 (0.301)   Data 0.242 (0.242)   Loss 0.1495 (0.1495)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:17:03]
  Epoch: [048][200/391]   Time 0.046 (0.045)   Data 0.000 (0.001)   Loss 0.2310 (0.2080)   Prec@1 89.844 (92.837)   Prec@5 100.000 (99.914)   [2018-11-14 11:17:11]
  **Train** Prec@1 92.396 Prec@5 99.876 Error@1 7.604
  **Test** Prec@1 82.690 Prec@5 99.170 Error@1 17.310
  **Test** Prec@1 82.690 Prec@5 99.170 Error@1 17.310

==>>[2018-11-14 11:17:24] [Epoch=049/200] [Need: 00:54:46] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [049][000/391]   Time 0.309 (0.309)   Data 0.248 (0.248)   Loss 0.1360 (0.1360)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:17:24]
  Epoch: [049][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.1470 (0.2050)   Prec@1 96.094 (92.938)   Prec@5 100.000 (99.895)   [2018-11-14 11:17:33]
  **Train** Prec@1 92.376 Prec@5 99.856 Error@1 7.624
  **Test** Prec@1 81.130 Prec@5 98.950 Error@1 18.870
  **Test** Prec@1 81.130 Prec@5 98.950 Error@1 18.870

==>>[2018-11-14 11:17:46] [Epoch=050/200] [Need: 00:54:25] [learning_rate=0.1000] [Best : Accuracy=85.27, Error=14.73]
  Epoch: [050][000/391]   Time 0.321 (0.321)   Data 0.256 (0.256)   Loss 0.1521 (0.1521)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:17:46]
  Epoch: [050][200/391]   Time 0.051 (0.046)   Data 0.000 (0.002)   Loss 0.1776 (0.2116)   Prec@1 93.750 (92.821)   Prec@5 100.000 (99.891)   [2018-11-14 11:17:55]
  **Train** Prec@1 92.418 Prec@5 99.870 Error@1 7.582
  **Test** Prec@1 86.380 Prec@5 99.440 Error@1 13.620
  **Test** Prec@1 86.380 Prec@5 99.440 Error@1 13.620

==>>[2018-11-14 11:18:08] [Epoch=051/200] [Need: 00:54:03] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [051][000/391]   Time 0.320 (0.320)   Data 0.258 (0.258)   Loss 0.1038 (0.1038)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-11-14 11:18:08]
  Epoch: [051][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.3000 (0.2032)   Prec@1 90.625 (92.965)   Prec@5 100.000 (99.891)   [2018-11-14 11:18:17]
  **Train** Prec@1 92.412 Prec@5 99.878 Error@1 7.588
  **Test** Prec@1 85.800 Prec@5 99.330 Error@1 14.200
  **Test** Prec@1 85.800 Prec@5 99.330 Error@1 14.200

==>>[2018-11-14 11:18:29] [Epoch=052/200] [Need: 00:53:41] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [052][000/391]   Time 0.312 (0.312)   Data 0.254 (0.254)   Loss 0.1527 (0.1527)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:18:29]
  Epoch: [052][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.2093 (0.2027)   Prec@1 94.531 (93.116)   Prec@5 100.000 (99.895)   [2018-11-14 11:18:38]
  **Train** Prec@1 92.402 Prec@5 99.866 Error@1 7.598
  **Test** Prec@1 85.300 Prec@5 99.260 Error@1 14.700
  **Test** Prec@1 85.300 Prec@5 99.260 Error@1 14.700

==>>[2018-11-14 11:18:51] [Epoch=053/200] [Need: 00:53:19] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [053][000/391]   Time 0.325 (0.325)   Data 0.272 (0.272)   Loss 0.1566 (0.1566)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:18:51]
  Epoch: [053][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.2208 (0.1973)   Prec@1 90.625 (93.280)   Prec@5 100.000 (99.876)   [2018-11-14 11:19:00]
  **Train** Prec@1 92.568 Prec@5 99.846 Error@1 7.432
  **Test** Prec@1 82.730 Prec@5 99.140 Error@1 17.270
  **Test** Prec@1 82.730 Prec@5 99.140 Error@1 17.270

==>>[2018-11-14 11:19:13] [Epoch=054/200] [Need: 00:52:57] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [054][000/391]   Time 0.307 (0.307)   Data 0.249 (0.249)   Loss 0.1409 (0.1409)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:19:13]
  Epoch: [054][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.3697 (0.2055)   Prec@1 85.938 (92.907)   Prec@5 100.000 (99.899)   [2018-11-14 11:19:22]
  **Train** Prec@1 92.410 Prec@5 99.852 Error@1 7.590
  **Test** Prec@1 83.910 Prec@5 99.110 Error@1 16.090
  **Test** Prec@1 83.910 Prec@5 99.110 Error@1 16.090

==>>[2018-11-14 11:19:34] [Epoch=055/200] [Need: 00:52:35] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [055][000/391]   Time 0.314 (0.314)   Data 0.258 (0.258)   Loss 0.2193 (0.2193)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:19:35]
  Epoch: [055][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.1656 (0.2027)   Prec@1 93.750 (93.046)   Prec@5 100.000 (99.907)   [2018-11-14 11:19:43]
  **Train** Prec@1 92.416 Prec@5 99.882 Error@1 7.584
  **Test** Prec@1 83.210 Prec@5 99.120 Error@1 16.790
  **Test** Prec@1 83.210 Prec@5 99.120 Error@1 16.790

==>>[2018-11-14 11:19:56] [Epoch=056/200] [Need: 00:52:13] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [056][000/391]   Time 0.308 (0.308)   Data 0.249 (0.249)   Loss 0.2360 (0.2360)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-11-14 11:19:56]
  Epoch: [056][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.2428 (0.2034)   Prec@1 88.281 (92.922)   Prec@5 100.000 (99.872)   [2018-11-14 11:20:05]
  **Train** Prec@1 92.476 Prec@5 99.844 Error@1 7.524
  **Test** Prec@1 84.950 Prec@5 99.230 Error@1 15.050
  **Test** Prec@1 84.950 Prec@5 99.230 Error@1 15.050

==>>[2018-11-14 11:20:18] [Epoch=057/200] [Need: 00:51:51] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [057][000/391]   Time 0.306 (0.306)   Data 0.247 (0.247)   Loss 0.1708 (0.1708)   Prec@1 92.188 (92.188)   Prec@5 100.000 (100.000)   [2018-11-14 11:20:18]
  Epoch: [057][200/391]   Time 0.045 (0.046)   Data 0.000 (0.001)   Loss 0.1913 (0.2030)   Prec@1 93.750 (93.179)   Prec@5 99.219 (99.895)   [2018-11-14 11:20:27]
  **Train** Prec@1 92.486 Prec@5 99.876 Error@1 7.514
  **Test** Prec@1 84.970 Prec@5 99.270 Error@1 15.030
  **Test** Prec@1 84.970 Prec@5 99.270 Error@1 15.030

==>>[2018-11-14 11:20:40] [Epoch=058/200] [Need: 00:51:30] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [058][000/391]   Time 0.306 (0.306)   Data 0.254 (0.254)   Loss 0.1357 (0.1357)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-11-14 11:20:40]
  Epoch: [058][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.2152 (0.2068)   Prec@1 90.625 (92.907)   Prec@5 100.000 (99.899)   [2018-11-14 11:20:49]
  **Train** Prec@1 92.360 Prec@5 99.870 Error@1 7.640
  **Test** Prec@1 81.830 Prec@5 98.800 Error@1 18.170
  **Test** Prec@1 81.830 Prec@5 98.800 Error@1 18.170

==>>[2018-11-14 11:21:01] [Epoch=059/200] [Need: 00:51:09] [learning_rate=0.1000] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [059][000/391]   Time 0.310 (0.310)   Data 0.251 (0.251)   Loss 0.2276 (0.2276)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-11-14 11:21:02]
  Epoch: [059][200/391]   Time 0.052 (0.046)   Data 0.000 (0.002)   Loss 0.1568 (0.2116)   Prec@1 93.750 (92.689)   Prec@5 99.219 (99.825)   [2018-11-14 11:21:11]
  **Train** Prec@1 92.368 Prec@5 99.830 Error@1 7.632
  **Test** Prec@1 83.350 Prec@5 98.820 Error@1 16.650
  **Test** Prec@1 83.350 Prec@5 98.820 Error@1 16.650

==>>[2018-11-14 11:21:23] [Epoch=060/200] [Need: 00:50:47] [learning_rate=0.0200] [Best : Accuracy=86.38, Error=13.62]
  Epoch: [060][000/391]   Time 0.302 (0.302)   Data 0.242 (0.242)   Loss 0.1972 (0.1972)   Prec@1 93.750 (93.750)   Prec@5 100.000 (100.000)   [2018-11-14 11:21:24]
  Epoch: [060][200/391]   Time 0.046 (0.046)   Data 0.000 (0.001)   Loss 0.0582 (0.0989)   Prec@1 97.656 (96.801)   Prec@5 100.000 (99.988)   [2018-11-14 11:21:33]
  **Train** Prec@1 97.462 Prec@5 99.990 Error@1 2.538
  **Test** Prec@1 91.400 Prec@5 99.790 Error@1 8.600
  **Test** Prec@1 91.400 Prec@5 99.790 Error@1 8.600

==>>[2018-11-14 11:21:45] [Epoch=061/200] [Need: 00:50:26] [learning_rate=0.0200] [Best : Accuracy=91.40, Error=8.60]
  Epoch: [061][000/391]   Time 0.316 (0.316)   Data 0.257 (0.257)   Loss 0.0395 (0.0395)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:21:46]
  Epoch: [061][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0240 (0.0354)   Prec@1 100.000 (99.005)   Prec@5 100.000 (99.996)   [2018-11-14 11:21:55]
  **Train** Prec@1 99.012 Prec@5 99.996 Error@1 0.988
  **Test** Prec@1 91.550 Prec@5 99.690 Error@1 8.450
  **Test** Prec@1 91.550 Prec@5 99.690 Error@1 8.450

==>>[2018-11-14 11:22:07] [Epoch=062/200] [Need: 00:50:04] [learning_rate=0.0200] [Best : Accuracy=91.55, Error=8.45]
  Epoch: [062][000/391]   Time 0.303 (0.303)   Data 0.245 (0.245)   Loss 0.0336 (0.0336)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:22:08]
  Epoch: [062][200/391]   Time 0.043 (0.046)   Data 0.000 (0.001)   Loss 0.0389 (0.0196)   Prec@1 98.438 (99.588)   Prec@5 100.000 (99.996)   [2018-11-14 11:22:17]
  **Train** Prec@1 99.582 Prec@5 99.996 Error@1 0.418
  **Test** Prec@1 91.520 Prec@5 99.690 Error@1 8.480
  **Test** Prec@1 91.520 Prec@5 99.690 Error@1 8.480

==>>[2018-11-14 11:22:29] [Epoch=063/200] [Need: 00:49:42] [learning_rate=0.0200] [Best : Accuracy=91.55, Error=8.45]
  Epoch: [063][000/391]   Time 0.324 (0.324)   Data 0.269 (0.269)   Loss 0.0273 (0.0273)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:22:29]
  Epoch: [063][200/391]   Time 0.044 (0.045)   Data 0.000 (0.002)   Loss 0.0236 (0.0135)   Prec@1 99.219 (99.743)   Prec@5 100.000 (99.996)   [2018-11-14 11:22:38]
  **Train** Prec@1 99.748 Prec@5 99.996 Error@1 0.252
  **Test** Prec@1 91.960 Prec@5 99.720 Error@1 8.040
  **Test** Prec@1 91.960 Prec@5 99.720 Error@1 8.040

==>>[2018-11-14 11:22:51] [Epoch=064/200] [Need: 00:49:20] [learning_rate=0.0200] [Best : Accuracy=91.96, Error=8.04]
  Epoch: [064][000/391]   Time 0.313 (0.313)   Data 0.255 (0.255)   Loss 0.0023 (0.0023)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:22:51]
  Epoch: [064][200/391]   Time 0.047 (0.047)   Data 0.000 (0.002)   Loss 0.0055 (0.0084)   Prec@1 100.000 (99.895)   Prec@5 100.000 (100.000)   [2018-11-14 11:23:00]
  **Train** Prec@1 99.870 Prec@5 100.000 Error@1 0.130
  **Test** Prec@1 91.790 Prec@5 99.740 Error@1 8.210
  **Test** Prec@1 91.790 Prec@5 99.740 Error@1 8.210

==>>[2018-11-14 11:23:13] [Epoch=065/200] [Need: 00:48:59] [learning_rate=0.0200] [Best : Accuracy=91.96, Error=8.04]
  Epoch: [065][000/391]   Time 0.312 (0.312)   Data 0.254 (0.254)   Loss 0.0055 (0.0055)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:23:13]
  Epoch: [065][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0045 (0.0071)   Prec@1 100.000 (99.914)   Prec@5 100.000 (100.000)   [2018-11-14 11:23:22]
  **Train** Prec@1 99.916 Prec@5 100.000 Error@1 0.084
  **Test** Prec@1 92.180 Prec@5 99.670 Error@1 7.820
  **Test** Prec@1 92.180 Prec@5 99.670 Error@1 7.820

==>>[2018-11-14 11:23:35] [Epoch=066/200] [Need: 00:48:38] [learning_rate=0.0200] [Best : Accuracy=92.18, Error=7.82]
  Epoch: [066][000/391]   Time 0.303 (0.303)   Data 0.247 (0.247)   Loss 0.0021 (0.0021)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:23:35]
  Epoch: [066][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0039 (0.0046)   Prec@1 100.000 (99.973)   Prec@5 100.000 (100.000)   [2018-11-14 11:23:44]
  **Train** Prec@1 99.976 Prec@5 100.000 Error@1 0.024
  **Test** Prec@1 92.010 Prec@5 99.740 Error@1 7.990
  **Test** Prec@1 92.010 Prec@5 99.740 Error@1 7.990

==>>[2018-11-14 11:23:56] [Epoch=067/200] [Need: 00:48:16] [learning_rate=0.0200] [Best : Accuracy=92.18, Error=7.82]
  Epoch: [067][000/391]   Time 0.303 (0.303)   Data 0.249 (0.249)   Loss 0.0052 (0.0052)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:23:57]
  Epoch: [067][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0045 (0.0043)   Prec@1 100.000 (99.969)   Prec@5 100.000 (100.000)   [2018-11-14 11:24:05]
  **Train** Prec@1 99.974 Prec@5 100.000 Error@1 0.026
  **Test** Prec@1 92.320 Prec@5 99.730 Error@1 7.680
  **Test** Prec@1 92.320 Prec@5 99.730 Error@1 7.680

==>>[2018-11-14 11:24:18] [Epoch=068/200] [Need: 00:47:54] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [068][000/391]   Time 0.311 (0.311)   Data 0.260 (0.260)   Loss 0.0059 (0.0059)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:24:18]
  Epoch: [068][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0039 (0.0036)   Prec@1 100.000 (99.988)   Prec@5 100.000 (100.000)   [2018-11-14 11:24:27]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 92.190 Prec@5 99.740 Error@1 7.810
  **Test** Prec@1 92.190 Prec@5 99.740 Error@1 7.810

==>>[2018-11-14 11:24:40] [Epoch=069/200] [Need: 00:47:32] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [069][000/391]   Time 0.312 (0.312)   Data 0.257 (0.257)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:24:40]
  Epoch: [069][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0032)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-11-14 11:24:49]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 92.110 Prec@5 99.700 Error@1 7.890
  **Test** Prec@1 92.110 Prec@5 99.700 Error@1 7.890

==>>[2018-11-14 11:25:02] [Epoch=070/200] [Need: 00:47:11] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [070][000/391]   Time 0.309 (0.309)   Data 0.251 (0.251)   Loss 0.0035 (0.0035)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:25:02]
  Epoch: [070][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0022 (0.0027)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:25:11]
  **Train** Prec@1 99.994 Prec@5 100.000 Error@1 0.006
  **Test** Prec@1 92.040 Prec@5 99.700 Error@1 7.960
  **Test** Prec@1 92.040 Prec@5 99.700 Error@1 7.960

==>>[2018-11-14 11:25:23] [Epoch=071/200] [Need: 00:46:49] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [071][000/391]   Time 0.306 (0.306)   Data 0.250 (0.250)   Loss 0.0027 (0.0027)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:25:24]
  Epoch: [071][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0026 (0.0029)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-11-14 11:25:33]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 92.260 Prec@5 99.710 Error@1 7.740
  **Test** Prec@1 92.260 Prec@5 99.710 Error@1 7.740

==>>[2018-11-14 11:25:45] [Epoch=072/200] [Need: 00:46:26] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [072][000/391]   Time 0.320 (0.320)   Data 0.271 (0.271)   Loss 0.0044 (0.0044)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:25:45]
  Epoch: [072][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0025 (0.0027)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:25:54]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.290 Prec@5 99.750 Error@1 7.710
  **Test** Prec@1 92.290 Prec@5 99.750 Error@1 7.710

==>>[2018-11-14 11:26:07] [Epoch=073/200] [Need: 00:46:05] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [073][000/391]   Time 0.302 (0.302)   Data 0.246 (0.246)   Loss 0.0033 (0.0033)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:26:07]
  Epoch: [073][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0023 (0.0026)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-11-14 11:26:16]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 92.220 Prec@5 99.790 Error@1 7.780
  **Test** Prec@1 92.220 Prec@5 99.790 Error@1 7.780

==>>[2018-11-14 11:26:28] [Epoch=074/200] [Need: 00:45:43] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [074][000/391]   Time 0.312 (0.312)   Data 0.253 (0.253)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:26:29]
  Epoch: [074][200/391]   Time 0.043 (0.045)   Data 0.000 (0.002)   Loss 0.0025 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:26:37]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.210 Prec@5 99.750 Error@1 7.790
  **Test** Prec@1 92.210 Prec@5 99.750 Error@1 7.790

==>>[2018-11-14 11:26:50] [Epoch=075/200] [Need: 00:45:20] [learning_rate=0.0200] [Best : Accuracy=92.32, Error=7.68]
  Epoch: [075][000/391]   Time 0.325 (0.325)   Data 0.268 (0.268)   Loss 0.0025 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:26:50]
  Epoch: [075][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0039 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:26:59]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.410 Prec@5 99.720 Error@1 7.590
  **Test** Prec@1 92.410 Prec@5 99.720 Error@1 7.590

==>>[2018-11-14 11:27:11] [Epoch=076/200] [Need: 00:44:58] [learning_rate=0.0200] [Best : Accuracy=92.41, Error=7.59]
  Epoch: [076][000/391]   Time 0.303 (0.303)   Data 0.243 (0.243)   Loss 0.0023 (0.0023)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:27:12]
  Epoch: [076][200/391]   Time 0.046 (0.046)   Data 0.000 (0.001)   Loss 0.0021 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:27:21]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.510 Prec@5 99.760 Error@1 7.490
  **Test** Prec@1 92.510 Prec@5 99.760 Error@1 7.490

==>>[2018-11-14 11:27:33] [Epoch=077/200] [Need: 00:44:37] [learning_rate=0.0200] [Best : Accuracy=92.51, Error=7.49]
  Epoch: [077][000/391]   Time 0.304 (0.304)   Data 0.254 (0.254)   Loss 0.0045 (0.0045)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:27:34]
  Epoch: [077][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0028 (0.0023)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:27:43]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 92.190 Prec@5 99.720 Error@1 7.810
  **Test** Prec@1 92.190 Prec@5 99.720 Error@1 7.810

==>>[2018-11-14 11:27:55] [Epoch=078/200] [Need: 00:44:15] [learning_rate=0.0200] [Best : Accuracy=92.51, Error=7.49]
  Epoch: [078][000/391]   Time 0.318 (0.318)   Data 0.261 (0.261)   Loss 0.0022 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:27:56]
  Epoch: [078][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0023 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:28:04]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.520 Prec@5 99.770 Error@1 7.480
  **Test** Prec@1 92.520 Prec@5 99.770 Error@1 7.480

==>>[2018-11-14 11:28:17] [Epoch=079/200] [Need: 00:43:54] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [079][000/391]   Time 0.318 (0.318)   Data 0.264 (0.264)   Loss 0.0021 (0.0021)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:28:18]
  Epoch: [079][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:28:26]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.400 Prec@5 99.740 Error@1 7.600
  **Test** Prec@1 92.400 Prec@5 99.740 Error@1 7.600

==>>[2018-11-14 11:28:39] [Epoch=080/200] [Need: 00:43:32] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [080][000/391]   Time 0.301 (0.301)   Data 0.246 (0.246)   Loss 0.0022 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:28:39]
  Epoch: [080][200/391]   Time 0.042 (0.045)   Data 0.000 (0.002)   Loss 0.0025 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:28:48]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.410 Prec@5 99.720 Error@1 7.590
  **Test** Prec@1 92.410 Prec@5 99.720 Error@1 7.590

==>>[2018-11-14 11:29:00] [Epoch=081/200] [Need: 00:43:09] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [081][000/391]   Time 0.307 (0.307)   Data 0.252 (0.252)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:29:00]
  Epoch: [081][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0039 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:29:09]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.430 Prec@5 99.690 Error@1 7.570
  **Test** Prec@1 92.430 Prec@5 99.690 Error@1 7.570

==>>[2018-11-14 11:29:21] [Epoch=082/200] [Need: 00:42:47] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [082][000/391]   Time 0.313 (0.313)   Data 0.260 (0.260)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:29:22]
  Epoch: [082][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0696 (0.0177)   Prec@1 99.219 (99.522)   Prec@5 100.000 (100.000)   [2018-11-14 11:29:31]
  **Train** Prec@1 97.046 Prec@5 99.974 Error@1 2.954
  **Test** Prec@1 84.950 Prec@5 99.170 Error@1 15.050
  **Test** Prec@1 84.950 Prec@5 99.170 Error@1 15.050

==>>[2018-11-14 11:29:43] [Epoch=083/200] [Need: 00:42:25] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [083][000/391]   Time 0.308 (0.308)   Data 0.251 (0.251)   Loss 0.2200 (0.2200)   Prec@1 92.188 (92.188)   Prec@5 99.219 (99.219)   [2018-11-14 11:29:44]
  Epoch: [083][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.1425 (0.1558)   Prec@1 96.094 (94.733)   Prec@5 99.219 (99.922)   [2018-11-14 11:29:52]
  **Train** Prec@1 94.656 Prec@5 99.938 Error@1 5.344
  **Test** Prec@1 84.110 Prec@5 99.160 Error@1 15.890
  **Test** Prec@1 84.110 Prec@5 99.160 Error@1 15.890

==>>[2018-11-14 11:30:05] [Epoch=084/200] [Need: 00:42:03] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [084][000/391]   Time 0.302 (0.302)   Data 0.251 (0.251)   Loss 0.1464 (0.1464)   Prec@1 95.312 (95.312)   Prec@5 100.000 (100.000)   [2018-11-14 11:30:05]
  Epoch: [084][200/391]   Time 0.040 (0.045)   Data 0.000 (0.002)   Loss 0.1013 (0.1090)   Prec@1 96.094 (96.319)   Prec@5 100.000 (99.981)   [2018-11-14 11:30:14]
  **Train** Prec@1 96.226 Prec@5 99.980 Error@1 3.774
  **Test** Prec@1 88.010 Prec@5 99.520 Error@1 11.990
  **Test** Prec@1 88.010 Prec@5 99.520 Error@1 11.990

==>>[2018-11-14 11:30:26] [Epoch=085/200] [Need: 00:41:41] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [085][000/391]   Time 0.308 (0.308)   Data 0.257 (0.257)   Loss 0.0874 (0.0874)   Prec@1 96.094 (96.094)   Prec@5 100.000 (100.000)   [2018-11-14 11:30:27]
  Epoch: [085][200/391]   Time 0.043 (0.047)   Data 0.000 (0.002)   Loss 0.0888 (0.0880)   Prec@1 96.094 (97.011)   Prec@5 100.000 (99.992)   [2018-11-14 11:30:36]
  **Train** Prec@1 96.796 Prec@5 99.984 Error@1 3.204
  **Test** Prec@1 88.330 Prec@5 99.470 Error@1 11.670
  **Test** Prec@1 88.330 Prec@5 99.470 Error@1 11.670

==>>[2018-11-14 11:30:48] [Epoch=086/200] [Need: 00:41:20] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [086][000/391]   Time 0.316 (0.316)   Data 0.262 (0.262)   Loss 0.0635 (0.0635)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:30:49]
  Epoch: [086][200/391]   Time 0.042 (0.047)   Data 0.000 (0.002)   Loss 0.0544 (0.0670)   Prec@1 98.438 (97.932)   Prec@5 100.000 (99.988)   [2018-11-14 11:30:58]
  **Train** Prec@1 97.620 Prec@5 99.992 Error@1 2.380
  **Test** Prec@1 88.320 Prec@5 99.490 Error@1 11.680
  **Test** Prec@1 88.320 Prec@5 99.490 Error@1 11.680

==>>[2018-11-14 11:31:10] [Epoch=087/200] [Need: 00:40:58] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [087][000/391]   Time 0.303 (0.303)   Data 0.250 (0.250)   Loss 0.0580 (0.0580)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:31:11]
  Epoch: [087][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.1076 (0.0572)   Prec@1 96.875 (98.231)   Prec@5 100.000 (99.988)   [2018-11-14 11:31:19]
  **Train** Prec@1 97.954 Prec@5 99.984 Error@1 2.046
  **Test** Prec@1 88.500 Prec@5 99.440 Error@1 11.500
  **Test** Prec@1 88.500 Prec@5 99.440 Error@1 11.500

==>>[2018-11-14 11:31:32] [Epoch=088/200] [Need: 00:40:36] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [088][000/391]   Time 0.312 (0.312)   Data 0.251 (0.251)   Loss 0.0416 (0.0416)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:31:32]
  Epoch: [088][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0686 (0.0567)   Prec@1 98.438 (98.173)   Prec@5 100.000 (99.992)   [2018-11-14 11:31:41]
  **Train** Prec@1 98.136 Prec@5 99.992 Error@1 1.864
  **Test** Prec@1 89.000 Prec@5 99.480 Error@1 11.000
  **Test** Prec@1 89.000 Prec@5 99.480 Error@1 11.000

==>>[2018-11-14 11:31:54] [Epoch=089/200] [Need: 00:40:15] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [089][000/391]   Time 0.320 (0.320)   Data 0.261 (0.261)   Loss 0.0285 (0.0285)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:31:54]
  Epoch: [089][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0606 (0.0508)   Prec@1 97.656 (98.352)   Prec@5 100.000 (100.000)   [2018-11-14 11:32:03]
  **Train** Prec@1 98.226 Prec@5 100.000 Error@1 1.774
  **Test** Prec@1 88.530 Prec@5 99.520 Error@1 11.470
  **Test** Prec@1 88.530 Prec@5 99.520 Error@1 11.470

==>>[2018-11-14 11:32:16] [Epoch=090/200] [Need: 00:39:53] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [090][000/391]   Time 0.320 (0.320)   Data 0.259 (0.259)   Loss 0.0332 (0.0332)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:32:16]
  Epoch: [090][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0333 (0.0492)   Prec@1 98.438 (98.511)   Prec@5 100.000 (99.988)   [2018-11-14 11:32:25]
  **Train** Prec@1 98.486 Prec@5 99.992 Error@1 1.514
  **Test** Prec@1 88.690 Prec@5 99.570 Error@1 11.310
  **Test** Prec@1 88.690 Prec@5 99.570 Error@1 11.310

==>>[2018-11-14 11:32:37] [Epoch=091/200] [Need: 00:39:31] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [091][000/391]   Time 0.305 (0.305)   Data 0.248 (0.248)   Loss 0.0453 (0.0453)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:32:38]
  Epoch: [091][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0469 (0.0398)   Prec@1 97.656 (98.717)   Prec@5 100.000 (100.000)   [2018-11-14 11:32:47]
  **Train** Prec@1 98.426 Prec@5 99.996 Error@1 1.574
  **Test** Prec@1 88.780 Prec@5 99.430 Error@1 11.220
  **Test** Prec@1 88.780 Prec@5 99.430 Error@1 11.220

==>>[2018-11-14 11:32:59] [Epoch=092/200] [Need: 00:39:09] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [092][000/391]   Time 0.303 (0.303)   Data 0.247 (0.247)   Loss 0.0431 (0.0431)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:32:59]
  Epoch: [092][200/391]   Time 0.043 (0.045)   Data 0.000 (0.001)   Loss 0.1273 (0.0539)   Prec@1 96.094 (98.356)   Prec@5 100.000 (99.988)   [2018-11-14 11:33:08]
  **Train** Prec@1 98.368 Prec@5 99.992 Error@1 1.632
  **Test** Prec@1 89.100 Prec@5 99.500 Error@1 10.900
  **Test** Prec@1 89.100 Prec@5 99.500 Error@1 10.900

==>>[2018-11-14 11:33:21] [Epoch=093/200] [Need: 00:38:47] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [093][000/391]   Time 0.309 (0.309)   Data 0.253 (0.253)   Loss 0.0265 (0.0265)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:33:21]
  Epoch: [093][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0595 (0.0398)   Prec@1 97.656 (98.811)   Prec@5 100.000 (99.996)   [2018-11-14 11:33:30]
  **Train** Prec@1 98.656 Prec@5 99.998 Error@1 1.344
  **Test** Prec@1 87.490 Prec@5 99.310 Error@1 12.510
  **Test** Prec@1 87.490 Prec@5 99.310 Error@1 12.510

==>>[2018-11-14 11:33:42] [Epoch=094/200] [Need: 00:38:26] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [094][000/391]   Time 0.328 (0.328)   Data 0.270 (0.270)   Loss 0.0115 (0.0115)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:33:43]
  Epoch: [094][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0388 (0.0376)   Prec@1 99.219 (98.842)   Prec@5 100.000 (100.000)   [2018-11-14 11:33:52]
  **Train** Prec@1 98.714 Prec@5 100.000 Error@1 1.286
  **Test** Prec@1 88.810 Prec@5 99.390 Error@1 11.190
  **Test** Prec@1 88.810 Prec@5 99.390 Error@1 11.190

==>>[2018-11-14 11:34:04] [Epoch=095/200] [Need: 00:38:04] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [095][000/391]   Time 0.310 (0.310)   Data 0.250 (0.250)   Loss 0.0678 (0.0678)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:34:05]
  Epoch: [095][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0310 (0.0391)   Prec@1 99.219 (98.745)   Prec@5 100.000 (100.000)   [2018-11-14 11:34:14]
  **Train** Prec@1 98.622 Prec@5 99.994 Error@1 1.378
  **Test** Prec@1 88.250 Prec@5 99.570 Error@1 11.750
  **Test** Prec@1 88.250 Prec@5 99.570 Error@1 11.750

==>>[2018-11-14 11:34:26] [Epoch=096/200] [Need: 00:37:42] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [096][000/391]   Time 0.312 (0.312)   Data 0.256 (0.256)   Loss 0.0623 (0.0623)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:34:26]
  Epoch: [096][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0620 (0.0435)   Prec@1 98.438 (98.581)   Prec@5 100.000 (99.996)   [2018-11-14 11:34:35]
  **Train** Prec@1 98.568 Prec@5 99.994 Error@1 1.432
  **Test** Prec@1 88.350 Prec@5 99.550 Error@1 11.650
  **Test** Prec@1 88.350 Prec@5 99.550 Error@1 11.650

==>>[2018-11-14 11:34:48] [Epoch=097/200] [Need: 00:37:21] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [097][000/391]   Time 0.296 (0.296)   Data 0.249 (0.249)   Loss 0.0381 (0.0381)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:34:48]
  Epoch: [097][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0368 (0.0355)   Prec@1 99.219 (98.954)   Prec@5 100.000 (100.000)   [2018-11-14 11:34:57]
  **Train** Prec@1 98.668 Prec@5 100.000 Error@1 1.332
  **Test** Prec@1 89.290 Prec@5 99.350 Error@1 10.710
  **Test** Prec@1 89.290 Prec@5 99.350 Error@1 10.710

==>>[2018-11-14 11:35:10] [Epoch=098/200] [Need: 00:36:59] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [098][000/391]   Time 0.304 (0.304)   Data 0.248 (0.248)   Loss 0.0102 (0.0102)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:35:10]
  Epoch: [098][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0198 (0.0391)   Prec@1 99.219 (98.818)   Prec@5 100.000 (99.996)   [2018-11-14 11:35:19]
  **Train** Prec@1 98.566 Prec@5 99.996 Error@1 1.434
  **Test** Prec@1 88.580 Prec@5 99.390 Error@1 11.420
  **Test** Prec@1 88.580 Prec@5 99.390 Error@1 11.420

==>>[2018-11-14 11:35:31] [Epoch=099/200] [Need: 00:36:37] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [099][000/391]   Time 0.304 (0.304)   Data 0.252 (0.252)   Loss 0.0293 (0.0293)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:35:32]
  Epoch: [099][200/391]   Time 0.048 (0.046)   Data 0.000 (0.002)   Loss 0.0300 (0.0391)   Prec@1 99.219 (98.764)   Prec@5 100.000 (100.000)   [2018-11-14 11:35:40]
  **Train** Prec@1 98.634 Prec@5 99.998 Error@1 1.366
  **Test** Prec@1 89.080 Prec@5 99.270 Error@1 10.920
  **Test** Prec@1 89.080 Prec@5 99.270 Error@1 10.920

==>>[2018-11-14 11:35:53] [Epoch=100/200] [Need: 00:36:15] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [100][000/391]   Time 0.326 (0.326)   Data 0.267 (0.267)   Loss 0.0189 (0.0189)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:35:53]
  Epoch: [100][200/391]   Time 0.049 (0.046)   Data 0.000 (0.002)   Loss 0.0355 (0.0364)   Prec@1 98.438 (98.803)   Prec@5 100.000 (100.000)   [2018-11-14 11:36:02]
  **Train** Prec@1 98.630 Prec@5 100.000 Error@1 1.370
  **Test** Prec@1 88.170 Prec@5 99.510 Error@1 11.830
  **Test** Prec@1 88.170 Prec@5 99.510 Error@1 11.830

==>>[2018-11-14 11:36:15] [Epoch=101/200] [Need: 00:35:53] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [101][000/391]   Time 0.297 (0.297)   Data 0.242 (0.242)   Loss 0.0828 (0.0828)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:36:15]
  Epoch: [101][200/391]   Time 0.045 (0.047)   Data 0.000 (0.001)   Loss 0.0255 (0.0326)   Prec@1 99.219 (99.071)   Prec@5 100.000 (99.996)   [2018-11-14 11:36:24]
  **Train** Prec@1 98.942 Prec@5 99.998 Error@1 1.058
  **Test** Prec@1 88.930 Prec@5 99.310 Error@1 11.070
  **Test** Prec@1 88.930 Prec@5 99.310 Error@1 11.070

==>>[2018-11-14 11:36:37] [Epoch=102/200] [Need: 00:35:32] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [102][000/391]   Time 0.316 (0.316)   Data 0.259 (0.259)   Loss 0.0206 (0.0206)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:36:37]
  Epoch: [102][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0255 (0.0428)   Prec@1 99.219 (98.589)   Prec@5 100.000 (99.996)   [2018-11-14 11:36:46]
  **Train** Prec@1 98.640 Prec@5 99.996 Error@1 1.360
  **Test** Prec@1 88.740 Prec@5 99.340 Error@1 11.260
  **Test** Prec@1 88.740 Prec@5 99.340 Error@1 11.260

==>>[2018-11-14 11:36:58] [Epoch=103/200] [Need: 00:35:10] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [103][000/391]   Time 0.306 (0.306)   Data 0.251 (0.251)   Loss 0.0334 (0.0334)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:36:59]
  Epoch: [103][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0414 (0.0378)   Prec@1 99.219 (98.853)   Prec@5 100.000 (99.996)   [2018-11-14 11:37:08]
  **Train** Prec@1 98.718 Prec@5 99.996 Error@1 1.282
  **Test** Prec@1 88.340 Prec@5 99.550 Error@1 11.660
  **Test** Prec@1 88.340 Prec@5 99.550 Error@1 11.660

==>>[2018-11-14 11:37:20] [Epoch=104/200] [Need: 00:34:48] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [104][000/391]   Time 0.310 (0.310)   Data 0.255 (0.255)   Loss 0.1249 (0.1249)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-11-14 11:37:20]
  Epoch: [104][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0259 (0.0329)   Prec@1 99.219 (98.993)   Prec@5 100.000 (100.000)   [2018-11-14 11:37:29]
  **Train** Prec@1 98.828 Prec@5 99.992 Error@1 1.172
  **Test** Prec@1 87.690 Prec@5 98.920 Error@1 12.310
  **Test** Prec@1 87.690 Prec@5 98.920 Error@1 12.310

==>>[2018-11-14 11:37:42] [Epoch=105/200] [Need: 00:34:27] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [105][000/391]   Time 0.309 (0.309)   Data 0.258 (0.258)   Loss 0.0432 (0.0432)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:37:42]
  Epoch: [105][200/391]   Time 0.046 (0.047)   Data 0.000 (0.002)   Loss 0.0523 (0.0370)   Prec@1 97.656 (98.865)   Prec@5 100.000 (99.992)   [2018-11-14 11:37:51]
  **Train** Prec@1 98.532 Prec@5 99.994 Error@1 1.468
  **Test** Prec@1 88.290 Prec@5 99.280 Error@1 11.710
  **Test** Prec@1 88.290 Prec@5 99.280 Error@1 11.710

==>>[2018-11-14 11:38:04] [Epoch=106/200] [Need: 00:34:05] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [106][000/391]   Time 0.302 (0.302)   Data 0.252 (0.252)   Loss 0.0602 (0.0602)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:38:04]
  Epoch: [106][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0159 (0.0376)   Prec@1 100.000 (98.799)   Prec@5 100.000 (100.000)   [2018-11-14 11:38:13]
  **Train** Prec@1 98.732 Prec@5 100.000 Error@1 1.268
  **Test** Prec@1 87.540 Prec@5 99.290 Error@1 12.460
  **Test** Prec@1 87.540 Prec@5 99.290 Error@1 12.460

==>>[2018-11-14 11:38:26] [Epoch=107/200] [Need: 00:33:43] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [107][000/391]   Time 0.335 (0.335)   Data 0.284 (0.284)   Loss 0.0224 (0.0224)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:38:26]
  Epoch: [107][200/391]   Time 0.044 (0.045)   Data 0.000 (0.002)   Loss 0.0574 (0.0388)   Prec@1 98.438 (98.729)   Prec@5 100.000 (99.992)   [2018-11-14 11:38:35]
  **Train** Prec@1 98.582 Prec@5 99.996 Error@1 1.418
  **Test** Prec@1 87.840 Prec@5 99.170 Error@1 12.160
  **Test** Prec@1 87.840 Prec@5 99.170 Error@1 12.160

==>>[2018-11-14 11:38:47] [Epoch=108/200] [Need: 00:33:21] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [108][000/391]   Time 0.300 (0.300)   Data 0.247 (0.247)   Loss 0.0382 (0.0382)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:38:47]
  Epoch: [108][200/391]   Time 0.051 (0.046)   Data 0.000 (0.001)   Loss 0.0164 (0.0404)   Prec@1 100.000 (98.686)   Prec@5 100.000 (99.996)   [2018-11-14 11:38:56]
  **Train** Prec@1 98.632 Prec@5 99.996 Error@1 1.368
  **Test** Prec@1 88.420 Prec@5 99.380 Error@1 11.580
  **Test** Prec@1 88.420 Prec@5 99.380 Error@1 11.580

==>>[2018-11-14 11:39:09] [Epoch=109/200] [Need: 00:32:59] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [109][000/391]   Time 0.322 (0.322)   Data 0.267 (0.267)   Loss 0.0561 (0.0561)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:39:09]
  Epoch: [109][200/391]   Time 0.044 (0.047)   Data 0.000 (0.002)   Loss 0.0439 (0.0427)   Prec@1 98.438 (98.539)   Prec@5 100.000 (100.000)   [2018-11-14 11:39:18]
  **Train** Prec@1 98.518 Prec@5 99.998 Error@1 1.482
  **Test** Prec@1 88.770 Prec@5 99.420 Error@1 11.230
  **Test** Prec@1 88.770 Prec@5 99.420 Error@1 11.230

==>>[2018-11-14 11:39:31] [Epoch=110/200] [Need: 00:32:38] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [110][000/391]   Time 0.319 (0.319)   Data 0.267 (0.267)   Loss 0.0158 (0.0158)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:39:31]
  Epoch: [110][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0141 (0.0343)   Prec@1 99.219 (98.966)   Prec@5 100.000 (99.996)   [2018-11-14 11:39:40]
  **Train** Prec@1 98.622 Prec@5 99.994 Error@1 1.378
  **Test** Prec@1 87.870 Prec@5 99.340 Error@1 12.130
  **Test** Prec@1 87.870 Prec@5 99.340 Error@1 12.130

==>>[2018-11-14 11:39:52] [Epoch=111/200] [Need: 00:32:16] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [111][000/391]   Time 0.318 (0.318)   Data 0.263 (0.263)   Loss 0.0447 (0.0447)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:39:53]
  Epoch: [111][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0573 (0.0432)   Prec@1 98.438 (98.651)   Prec@5 100.000 (99.996)   [2018-11-14 11:40:02]
  **Train** Prec@1 98.596 Prec@5 99.996 Error@1 1.404
  **Test** Prec@1 88.250 Prec@5 99.350 Error@1 11.750
  **Test** Prec@1 88.250 Prec@5 99.350 Error@1 11.750

==>>[2018-11-14 11:40:14] [Epoch=112/200] [Need: 00:31:54] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [112][000/391]   Time 0.313 (0.313)   Data 0.253 (0.253)   Loss 0.0473 (0.0473)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:40:14]
  Epoch: [112][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.0211 (0.0338)   Prec@1 99.219 (98.970)   Prec@5 100.000 (99.992)   [2018-11-14 11:40:23]
  **Train** Prec@1 98.908 Prec@5 99.996 Error@1 1.092
  **Test** Prec@1 88.720 Prec@5 99.470 Error@1 11.280
  **Test** Prec@1 88.720 Prec@5 99.470 Error@1 11.280

==>>[2018-11-14 11:40:36] [Epoch=113/200] [Need: 00:31:32] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [113][000/391]   Time 0.306 (0.306)   Data 0.247 (0.247)   Loss 0.0257 (0.0257)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:40:36]
  Epoch: [113][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0243 (0.0370)   Prec@1 99.219 (98.853)   Prec@5 100.000 (99.992)   [2018-11-14 11:40:45]
  **Train** Prec@1 98.876 Prec@5 99.992 Error@1 1.124
  **Test** Prec@1 88.520 Prec@5 99.320 Error@1 11.480
  **Test** Prec@1 88.520 Prec@5 99.320 Error@1 11.480

==>>[2018-11-14 11:40:58] [Epoch=114/200] [Need: 00:31:11] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [114][000/391]   Time 0.341 (0.341)   Data 0.284 (0.284)   Loss 0.0135 (0.0135)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:40:58]
  Epoch: [114][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.0233 (0.0367)   Prec@1 99.219 (98.834)   Prec@5 100.000 (100.000)   [2018-11-14 11:41:07]
  **Train** Prec@1 98.800 Prec@5 100.000 Error@1 1.200
  **Test** Prec@1 88.740 Prec@5 99.510 Error@1 11.260
  **Test** Prec@1 88.740 Prec@5 99.510 Error@1 11.260

==>>[2018-11-14 11:41:19] [Epoch=115/200] [Need: 00:30:49] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [115][000/391]   Time 0.305 (0.305)   Data 0.249 (0.249)   Loss 0.0426 (0.0426)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:41:20]
  Epoch: [115][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0264 (0.0313)   Prec@1 99.219 (99.017)   Prec@5 100.000 (99.996)   [2018-11-14 11:41:29]
  **Train** Prec@1 98.852 Prec@5 99.998 Error@1 1.148
  **Test** Prec@1 88.220 Prec@5 99.450 Error@1 11.780
  **Test** Prec@1 88.220 Prec@5 99.450 Error@1 11.780

==>>[2018-11-14 11:41:41] [Epoch=116/200] [Need: 00:30:27] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [116][000/391]   Time 0.305 (0.305)   Data 0.258 (0.258)   Loss 0.0291 (0.0291)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:41:41]
  Epoch: [116][200/391]   Time 0.048 (0.047)   Data 0.000 (0.002)   Loss 0.0327 (0.0380)   Prec@1 98.438 (98.729)   Prec@5 100.000 (99.996)   [2018-11-14 11:41:50]
  **Train** Prec@1 98.616 Prec@5 99.998 Error@1 1.384
  **Test** Prec@1 88.490 Prec@5 99.370 Error@1 11.510
  **Test** Prec@1 88.490 Prec@5 99.370 Error@1 11.510

==>>[2018-11-14 11:42:03] [Epoch=117/200] [Need: 00:30:06] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [117][000/391]   Time 0.316 (0.316)   Data 0.258 (0.258)   Loss 0.0679 (0.0679)   Prec@1 96.875 (96.875)   Prec@5 100.000 (100.000)   [2018-11-14 11:42:03]
  Epoch: [117][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0632 (0.0381)   Prec@1 97.656 (98.745)   Prec@5 100.000 (100.000)   [2018-11-14 11:42:12]
  **Train** Prec@1 98.642 Prec@5 100.000 Error@1 1.358
  **Test** Prec@1 87.500 Prec@5 99.470 Error@1 12.500
  **Test** Prec@1 87.500 Prec@5 99.470 Error@1 12.500

==>>[2018-11-14 11:42:25] [Epoch=118/200] [Need: 00:29:44] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [118][000/391]   Time 0.302 (0.302)   Data 0.253 (0.253)   Loss 0.0305 (0.0305)   Prec@1 99.219 (99.219)   Prec@5 100.000 (100.000)   [2018-11-14 11:42:25]
  Epoch: [118][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.0251 (0.0386)   Prec@1 99.219 (98.733)   Prec@5 100.000 (100.000)   [2018-11-14 11:42:34]
  **Train** Prec@1 98.658 Prec@5 99.996 Error@1 1.342
  **Test** Prec@1 87.510 Prec@5 99.360 Error@1 12.490
  **Test** Prec@1 87.510 Prec@5 99.360 Error@1 12.490

==>>[2018-11-14 11:42:47] [Epoch=119/200] [Need: 00:29:22] [learning_rate=0.0200] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [119][000/391]   Time 0.306 (0.306)   Data 0.247 (0.247)   Loss 0.0427 (0.0427)   Prec@1 98.438 (98.438)   Prec@5 100.000 (100.000)   [2018-11-14 11:42:47]
  Epoch: [119][200/391]   Time 0.044 (0.044)   Data 0.000 (0.001)   Loss 0.0168 (0.0445)   Prec@1 100.000 (98.519)   Prec@5 100.000 (100.000)   [2018-11-14 11:42:56]
  **Train** Prec@1 98.490 Prec@5 99.998 Error@1 1.510
  **Test** Prec@1 87.680 Prec@5 99.470 Error@1 12.320
  **Test** Prec@1 87.680 Prec@5 99.470 Error@1 12.320

==>>[2018-11-14 11:43:08] [Epoch=120/200] [Need: 00:29:00] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [120][000/391]   Time 0.316 (0.316)   Data 0.256 (0.256)   Loss 0.0772 (0.0772)   Prec@1 97.656 (97.656)   Prec@5 100.000 (100.000)   [2018-11-14 11:43:08]
  Epoch: [120][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0037 (0.0199)   Prec@1 100.000 (99.456)   Prec@5 100.000 (100.000)   [2018-11-14 11:43:17]
  **Train** Prec@1 99.604 Prec@5 100.000 Error@1 0.396
  **Test** Prec@1 91.260 Prec@5 99.650 Error@1 8.740
  **Test** Prec@1 91.260 Prec@5 99.650 Error@1 8.740

==>>[2018-11-14 11:43:30] [Epoch=121/200] [Need: 00:28:38] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [121][000/391]   Time 0.318 (0.318)   Data 0.262 (0.262)   Loss 0.0090 (0.0090)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:43:30]
  Epoch: [121][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0047 (0.0060)   Prec@1 100.000 (99.903)   Prec@5 100.000 (100.000)   [2018-11-14 11:43:39]
  **Train** Prec@1 99.910 Prec@5 100.000 Error@1 0.090
  **Test** Prec@1 91.590 Prec@5 99.700 Error@1 8.410
  **Test** Prec@1 91.590 Prec@5 99.700 Error@1 8.410

==>>[2018-11-14 11:43:51] [Epoch=122/200] [Need: 00:28:16] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [122][000/391]   Time 0.312 (0.312)   Data 0.249 (0.249)   Loss 0.0033 (0.0033)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:43:52]
  Epoch: [122][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0024 (0.0043)   Prec@1 100.000 (99.934)   Prec@5 100.000 (100.000)   [2018-11-14 11:44:01]
  **Train** Prec@1 99.944 Prec@5 100.000 Error@1 0.056
  **Test** Prec@1 91.900 Prec@5 99.730 Error@1 8.100
  **Test** Prec@1 91.900 Prec@5 99.730 Error@1 8.100

==>>[2018-11-14 11:44:13] [Epoch=123/200] [Need: 00:27:55] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [123][000/391]   Time 0.303 (0.303)   Data 0.250 (0.250)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:44:13]
  Epoch: [123][200/391]   Time 0.044 (0.045)   Data 0.000 (0.002)   Loss 0.0019 (0.0030)   Prec@1 100.000 (99.977)   Prec@5 100.000 (100.000)   [2018-11-14 11:44:22]
  **Train** Prec@1 99.976 Prec@5 100.000 Error@1 0.024
  **Test** Prec@1 91.830 Prec@5 99.720 Error@1 8.170
  **Test** Prec@1 91.830 Prec@5 99.720 Error@1 8.170

==>>[2018-11-14 11:44:35] [Epoch=124/200] [Need: 00:27:33] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [124][000/391]   Time 0.309 (0.309)   Data 0.250 (0.250)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:44:35]
  Epoch: [124][200/391]   Time 0.044 (0.045)   Data 0.000 (0.002)   Loss 0.0044 (0.0023)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-11-14 11:44:44]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 91.810 Prec@5 99.710 Error@1 8.190
  **Test** Prec@1 91.810 Prec@5 99.710 Error@1 8.190

==>>[2018-11-14 11:44:56] [Epoch=125/200] [Need: 00:27:11] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [125][000/391]   Time 0.316 (0.316)   Data 0.256 (0.256)   Loss 0.0012 (0.0012)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:44:56]
  Epoch: [125][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-11-14 11:45:05]
  **Train** Prec@1 99.990 Prec@5 100.000 Error@1 0.010
  **Test** Prec@1 91.990 Prec@5 99.720 Error@1 8.010
  **Test** Prec@1 91.990 Prec@5 99.720 Error@1 8.010

==>>[2018-11-14 11:45:18] [Epoch=126/200] [Need: 00:26:49] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [126][000/391]   Time 0.311 (0.311)   Data 0.255 (0.255)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:45:18]
  Epoch: [126][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:45:27]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 91.940 Prec@5 99.720 Error@1 8.060
  **Test** Prec@1 91.940 Prec@5 99.720 Error@1 8.060

==>>[2018-11-14 11:45:39] [Epoch=127/200] [Need: 00:26:27] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [127][000/391]   Time 0.317 (0.317)   Data 0.258 (0.258)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:45:40]
  Epoch: [127][200/391]   Time 0.043 (0.045)   Data 0.000 (0.002)   Loss 0.0021 (0.0018)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-11-14 11:45:49]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 92.050 Prec@5 99.750 Error@1 7.950
  **Test** Prec@1 92.050 Prec@5 99.750 Error@1 7.950

==>>[2018-11-14 11:46:01] [Epoch=128/200] [Need: 00:26:05] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [128][000/391]   Time 0.312 (0.312)   Data 0.257 (0.257)   Loss 0.0036 (0.0036)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:46:01]
  Epoch: [128][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0019 (0.0017)   Prec@1 100.000 (99.992)   Prec@5 100.000 (100.000)   [2018-11-14 11:46:10]
  **Train** Prec@1 99.996 Prec@5 100.000 Error@1 0.004
  **Test** Prec@1 92.030 Prec@5 99.690 Error@1 7.970
  **Test** Prec@1 92.030 Prec@5 99.690 Error@1 7.970

==>>[2018-11-14 11:46:23] [Epoch=129/200] [Need: 00:25:44] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [129][000/391]   Time 0.314 (0.314)   Data 0.254 (0.254)   Loss 0.0012 (0.0012)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:46:23]
  Epoch: [129][200/391]   Time 0.047 (0.047)   Data 0.000 (0.002)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:46:32]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 91.980 Prec@5 99.680 Error@1 8.020
  **Test** Prec@1 91.980 Prec@5 99.680 Error@1 8.020

==>>[2018-11-14 11:46:45] [Epoch=130/200] [Need: 00:25:22] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [130][000/391]   Time 0.314 (0.314)   Data 0.256 (0.256)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:46:45]
  Epoch: [130][200/391]   Time 0.048 (0.046)   Data 0.000 (0.002)   Loss 0.0014 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:46:54]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.200 Prec@5 99.710 Error@1 7.800
  **Test** Prec@1 92.200 Prec@5 99.710 Error@1 7.800

==>>[2018-11-14 11:47:07] [Epoch=131/200] [Need: 00:25:00] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [131][000/391]   Time 0.311 (0.311)   Data 0.253 (0.253)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:47:07]
  Epoch: [131][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.0015 (0.0016)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-11-14 11:47:16]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 92.180 Prec@5 99.690 Error@1 7.820
  **Test** Prec@1 92.180 Prec@5 99.690 Error@1 7.820

==>>[2018-11-14 11:47:28] [Epoch=132/200] [Need: 00:24:39] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [132][000/391]   Time 0.309 (0.309)   Data 0.248 (0.248)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:47:29]
  Epoch: [132][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0015 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:47:38]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.210 Prec@5 99.700 Error@1 7.790
  **Test** Prec@1 92.210 Prec@5 99.700 Error@1 7.790

==>>[2018-11-14 11:47:50] [Epoch=133/200] [Need: 00:24:17] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [133][000/391]   Time 0.315 (0.315)   Data 0.263 (0.263)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:47:51]
  Epoch: [133][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0011 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:47:59]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.250 Prec@5 99.730 Error@1 7.750
  **Test** Prec@1 92.250 Prec@5 99.730 Error@1 7.750

==>>[2018-11-14 11:48:12] [Epoch=134/200] [Need: 00:23:55] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [134][000/391]   Time 0.334 (0.334)   Data 0.283 (0.283)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:48:12]
  Epoch: [134][200/391]   Time 0.041 (0.047)   Data 0.000 (0.002)   Loss 0.0017 (0.0015)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:48:21]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.720 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.720 Error@1 7.720

==>>[2018-11-14 11:48:34] [Epoch=135/200] [Need: 00:23:33] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [135][000/391]   Time 0.313 (0.313)   Data 0.255 (0.255)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:48:34]
  Epoch: [135][200/391]   Time 0.050 (0.046)   Data 0.000 (0.002)   Loss 0.0022 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:48:43]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.270 Prec@5 99.690 Error@1 7.730
  **Test** Prec@1 92.270 Prec@5 99.690 Error@1 7.730

==>>[2018-11-14 11:48:56] [Epoch=136/200] [Need: 00:23:12] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [136][000/391]   Time 0.316 (0.316)   Data 0.259 (0.259)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:48:56]
  Epoch: [136][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0013 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:49:05]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.190 Prec@5 99.680 Error@1 7.810
  **Test** Prec@1 92.190 Prec@5 99.680 Error@1 7.810

==>>[2018-11-14 11:49:17] [Epoch=137/200] [Need: 00:22:50] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [137][000/391]   Time 0.313 (0.313)   Data 0.253 (0.253)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:49:18]
  Epoch: [137][200/391]   Time 0.079 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:49:27]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.290 Prec@5 99.740 Error@1 7.710
  **Test** Prec@1 92.290 Prec@5 99.740 Error@1 7.710

==>>[2018-11-14 11:49:39] [Epoch=138/200] [Need: 00:22:28] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [138][000/391]   Time 0.331 (0.331)   Data 0.273 (0.273)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:49:39]
  Epoch: [138][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:49:48]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.270 Prec@5 99.680 Error@1 7.730
  **Test** Prec@1 92.270 Prec@5 99.680 Error@1 7.730

==>>[2018-11-14 11:50:01] [Epoch=139/200] [Need: 00:22:06] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [139][000/391]   Time 0.299 (0.299)   Data 0.245 (0.245)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:50:01]
  Epoch: [139][200/391]   Time 0.046 (0.047)   Data 0.000 (0.002)   Loss 0.0013 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:50:10]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.250 Prec@5 99.730 Error@1 7.750
  **Test** Prec@1 92.250 Prec@5 99.730 Error@1 7.750

==>>[2018-11-14 11:50:23] [Epoch=140/200] [Need: 00:21:45] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [140][000/391]   Time 0.309 (0.309)   Data 0.254 (0.254)   Loss 0.0011 (0.0011)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:50:23]
  Epoch: [140][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0019 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:50:32]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.710 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.710 Error@1 7.720

==>>[2018-11-14 11:50:45] [Epoch=141/200] [Need: 00:21:23] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [141][000/391]   Time 0.326 (0.326)   Data 0.268 (0.268)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:50:45]
  Epoch: [141][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0012 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:50:54]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.720 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.720 Error@1 7.720

==>>[2018-11-14 11:51:06] [Epoch=142/200] [Need: 00:21:01] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [142][000/391]   Time 0.299 (0.299)   Data 0.251 (0.251)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:51:07]
  Epoch: [142][200/391]   Time 0.045 (0.047)   Data 0.000 (0.002)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:51:16]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.150 Prec@5 99.700 Error@1 7.850
  **Test** Prec@1 92.150 Prec@5 99.700 Error@1 7.850

==>>[2018-11-14 11:51:28] [Epoch=143/200] [Need: 00:20:40] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [143][000/391]   Time 0.319 (0.319)   Data 0.259 (0.259)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:51:29]
  Epoch: [143][200/391]   Time 0.046 (0.047)   Data 0.000 (0.002)   Loss 0.0013 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:51:38]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.360 Prec@5 99.700 Error@1 7.640
  **Test** Prec@1 92.360 Prec@5 99.700 Error@1 7.640

==>>[2018-11-14 11:51:50] [Epoch=144/200] [Need: 00:20:18] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [144][000/391]   Time 0.312 (0.312)   Data 0.255 (0.255)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:51:51]
  Epoch: [144][200/391]   Time 0.049 (0.046)   Data 0.000 (0.002)   Loss 0.0021 (0.0018)   Prec@1 100.000 (99.996)   Prec@5 100.000 (100.000)   [2018-11-14 11:51:59]
  **Train** Prec@1 99.998 Prec@5 100.000 Error@1 0.002
  **Test** Prec@1 92.330 Prec@5 99.680 Error@1 7.670
  **Test** Prec@1 92.330 Prec@5 99.680 Error@1 7.670

==>>[2018-11-14 11:52:12] [Epoch=145/200] [Need: 00:19:56] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [145][000/391]   Time 0.318 (0.318)   Data 0.259 (0.259)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:52:12]
  Epoch: [145][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:52:21]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.340 Prec@5 99.670 Error@1 7.660
  **Test** Prec@1 92.340 Prec@5 99.670 Error@1 7.660

==>>[2018-11-14 11:52:34] [Epoch=146/200] [Need: 00:19:34] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [146][000/391]   Time 0.315 (0.315)   Data 0.257 (0.257)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:52:34]
  Epoch: [146][200/391]   Time 0.048 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:52:43]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.350 Prec@5 99.700 Error@1 7.650
  **Test** Prec@1 92.350 Prec@5 99.700 Error@1 7.650

==>>[2018-11-14 11:52:56] [Epoch=147/200] [Need: 00:19:13] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [147][000/391]   Time 0.304 (0.304)   Data 0.248 (0.248)   Loss 0.0015 (0.0015)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:52:56]
  Epoch: [147][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.0015 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:53:05]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.300 Prec@5 99.700 Error@1 7.700
  **Test** Prec@1 92.300 Prec@5 99.700 Error@1 7.700

==>>[2018-11-14 11:53:18] [Epoch=148/200] [Need: 00:18:51] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [148][000/391]   Time 0.312 (0.312)   Data 0.256 (0.256)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:53:18]
  Epoch: [148][200/391]   Time 0.044 (0.044)   Data 0.000 (0.002)   Loss 0.0016 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:53:26]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.400 Prec@5 99.700 Error@1 7.600
  **Test** Prec@1 92.400 Prec@5 99.700 Error@1 7.600

==>>[2018-11-14 11:53:39] [Epoch=149/200] [Need: 00:18:29] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [149][000/391]   Time 0.321 (0.321)   Data 0.264 (0.264)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:53:39]
  Epoch: [149][200/391]   Time 0.044 (0.045)   Data 0.000 (0.002)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:53:48]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.690 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.690 Error@1 7.720

==>>[2018-11-14 11:54:00] [Epoch=150/200] [Need: 00:18:07] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [150][000/391]   Time 0.330 (0.330)   Data 0.271 (0.271)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:54:01]
  Epoch: [150][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:54:09]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.370 Prec@5 99.740 Error@1 7.630
  **Test** Prec@1 92.370 Prec@5 99.740 Error@1 7.630

==>>[2018-11-14 11:54:22] [Epoch=151/200] [Need: 00:17:45] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [151][000/391]   Time 0.310 (0.310)   Data 0.251 (0.251)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:54:22]
  Epoch: [151][200/391]   Time 0.047 (0.047)   Data 0.000 (0.002)   Loss 0.0020 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:54:31]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.350 Prec@5 99.700 Error@1 7.650
  **Test** Prec@1 92.350 Prec@5 99.700 Error@1 7.650

==>>[2018-11-14 11:54:44] [Epoch=152/200] [Need: 00:17:24] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [152][000/391]   Time 0.318 (0.318)   Data 0.260 (0.260)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:54:44]
  Epoch: [152][200/391]   Time 0.044 (0.047)   Data 0.000 (0.002)   Loss 0.0015 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:54:53]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.670 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.670 Error@1 7.720

==>>[2018-11-14 11:55:06] [Epoch=153/200] [Need: 00:17:02] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [153][000/391]   Time 0.301 (0.301)   Data 0.247 (0.247)   Loss 0.0028 (0.0028)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:55:06]
  Epoch: [153][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:55:15]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.370 Prec@5 99.730 Error@1 7.630
  **Test** Prec@1 92.370 Prec@5 99.730 Error@1 7.630

==>>[2018-11-14 11:55:28] [Epoch=154/200] [Need: 00:16:40] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [154][000/391]   Time 0.304 (0.304)   Data 0.245 (0.245)   Loss 0.0014 (0.0014)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:55:28]
  Epoch: [154][200/391]   Time 0.046 (0.046)   Data 0.000 (0.001)   Loss 0.0018 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:55:37]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.360 Prec@5 99.720 Error@1 7.640
  **Test** Prec@1 92.360 Prec@5 99.720 Error@1 7.640

==>>[2018-11-14 11:55:49] [Epoch=155/200] [Need: 00:16:18] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [155][000/391]   Time 0.313 (0.313)   Data 0.255 (0.255)   Loss 0.0024 (0.0024)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:55:50]
  Epoch: [155][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:55:59]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.690 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.690 Error@1 7.720

==>>[2018-11-14 11:56:11] [Epoch=156/200] [Need: 00:15:57] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [156][000/391]   Time 0.321 (0.321)   Data 0.261 (0.261)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:56:12]
  Epoch: [156][200/391]   Time 0.050 (0.047)   Data 0.000 (0.002)   Loss 0.0018 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:56:21]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.470 Prec@5 99.730 Error@1 7.530
  **Test** Prec@1 92.470 Prec@5 99.730 Error@1 7.530

==>>[2018-11-14 11:56:33] [Epoch=157/200] [Need: 00:15:35] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [157][000/391]   Time 0.311 (0.311)   Data 0.253 (0.253)   Loss 0.0013 (0.0013)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:56:34]
  Epoch: [157][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0015 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:56:43]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.250 Prec@5 99.720 Error@1 7.750
  **Test** Prec@1 92.250 Prec@5 99.720 Error@1 7.750

==>>[2018-11-14 11:56:55] [Epoch=158/200] [Need: 00:15:13] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [158][000/391]   Time 0.327 (0.327)   Data 0.273 (0.273)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:56:56]
  Epoch: [158][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:57:04]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.390 Prec@5 99.730 Error@1 7.610
  **Test** Prec@1 92.390 Prec@5 99.730 Error@1 7.610

==>>[2018-11-14 11:57:17] [Epoch=159/200] [Need: 00:14:52] [learning_rate=0.0040] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [159][000/391]   Time 0.358 (0.358)   Data 0.304 (0.304)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:57:17]
  Epoch: [159][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:57:26]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.340 Prec@5 99.690 Error@1 7.660
  **Test** Prec@1 92.340 Prec@5 99.690 Error@1 7.660

==>>[2018-11-14 11:57:39] [Epoch=160/200] [Need: 00:14:30] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [160][000/391]   Time 0.313 (0.313)   Data 0.257 (0.257)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:57:39]
  Epoch: [160][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0030 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:57:48]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.370 Prec@5 99.710 Error@1 7.630
  **Test** Prec@1 92.370 Prec@5 99.710 Error@1 7.630

==>>[2018-11-14 11:58:00] [Epoch=161/200] [Need: 00:14:08] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [161][000/391]   Time 0.318 (0.318)   Data 0.260 (0.260)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:58:01]
  Epoch: [161][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:58:10]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.420 Prec@5 99.700 Error@1 7.580
  **Test** Prec@1 92.420 Prec@5 99.700 Error@1 7.580

==>>[2018-11-14 11:58:22] [Epoch=162/200] [Need: 00:13:46] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [162][000/391]   Time 0.314 (0.314)   Data 0.257 (0.257)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:58:23]
  Epoch: [162][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0024 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:58:32]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.360 Prec@5 99.700 Error@1 7.640
  **Test** Prec@1 92.360 Prec@5 99.700 Error@1 7.640

==>>[2018-11-14 11:58:44] [Epoch=163/200] [Need: 00:13:25] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [163][000/391]   Time 0.333 (0.333)   Data 0.280 (0.280)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:58:44]
  Epoch: [163][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:58:53]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.400 Prec@5 99.700 Error@1 7.600
  **Test** Prec@1 92.400 Prec@5 99.700 Error@1 7.600

==>>[2018-11-14 11:59:06] [Epoch=164/200] [Need: 00:13:03] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [164][000/391]   Time 0.308 (0.308)   Data 0.259 (0.259)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:59:06]
  Epoch: [164][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:59:15]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.460 Prec@5 99.710 Error@1 7.540
  **Test** Prec@1 92.460 Prec@5 99.710 Error@1 7.540

==>>[2018-11-14 11:59:28] [Epoch=165/200] [Need: 00:12:41] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [165][000/391]   Time 0.309 (0.309)   Data 0.264 (0.264)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:59:28]
  Epoch: [165][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:59:37]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.450 Prec@5 99.710 Error@1 7.550
  **Test** Prec@1 92.450 Prec@5 99.710 Error@1 7.550

==>>[2018-11-14 11:59:49] [Epoch=166/200] [Need: 00:12:19] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [166][000/391]   Time 0.309 (0.309)   Data 0.253 (0.253)   Loss 0.0024 (0.0024)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:59:50]
  Epoch: [166][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0024 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 11:59:59]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.390 Prec@5 99.730 Error@1 7.610
  **Test** Prec@1 92.390 Prec@5 99.730 Error@1 7.610

==>>[2018-11-14 12:00:11] [Epoch=167/200] [Need: 00:11:58] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [167][000/391]   Time 0.319 (0.319)   Data 0.262 (0.262)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:00:11]
  Epoch: [167][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:00:20]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.210 Prec@5 99.720 Error@1 7.790
  **Test** Prec@1 92.210 Prec@5 99.720 Error@1 7.790

==>>[2018-11-14 12:00:33] [Epoch=168/200] [Need: 00:11:36] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [168][000/391]   Time 0.319 (0.319)   Data 0.267 (0.267)   Loss 0.0022 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:00:33]
  Epoch: [168][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:00:42]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.380 Prec@5 99.710 Error@1 7.620
  **Test** Prec@1 92.380 Prec@5 99.710 Error@1 7.620

==>>[2018-11-14 12:00:55] [Epoch=169/200] [Need: 00:11:14] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [169][000/391]   Time 0.322 (0.322)   Data 0.278 (0.278)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:00:55]
  Epoch: [169][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0014 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:01:04]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.390 Prec@5 99.740 Error@1 7.610
  **Test** Prec@1 92.390 Prec@5 99.740 Error@1 7.610

==>>[2018-11-14 12:01:17] [Epoch=170/200] [Need: 00:10:52] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [170][000/391]   Time 0.332 (0.332)   Data 0.273 (0.273)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:01:17]
  Epoch: [170][200/391]   Time 0.046 (0.047)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:01:26]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.390 Prec@5 99.730 Error@1 7.610
  **Test** Prec@1 92.390 Prec@5 99.730 Error@1 7.610

==>>[2018-11-14 12:01:39] [Epoch=171/200] [Need: 00:10:31] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [171][000/391]   Time 0.309 (0.309)   Data 0.251 (0.251)   Loss 0.0023 (0.0023)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:01:39]
  Epoch: [171][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:01:48]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.320 Prec@5 99.700 Error@1 7.680
  **Test** Prec@1 92.320 Prec@5 99.700 Error@1 7.680

==>>[2018-11-14 12:02:00] [Epoch=172/200] [Need: 00:10:09] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [172][000/391]   Time 0.345 (0.345)   Data 0.291 (0.291)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:02:01]
  Epoch: [172][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0021 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:02:10]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.380 Prec@5 99.740 Error@1 7.620
  **Test** Prec@1 92.380 Prec@5 99.740 Error@1 7.620

==>>[2018-11-14 12:02:22] [Epoch=173/200] [Need: 00:09:47] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [173][000/391]   Time 0.338 (0.338)   Data 0.278 (0.278)   Loss 0.0025 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:02:23]
  Epoch: [173][200/391]   Time 0.040 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:02:31]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.410 Prec@5 99.730 Error@1 7.590
  **Test** Prec@1 92.410 Prec@5 99.730 Error@1 7.590

==>>[2018-11-14 12:02:44] [Epoch=174/200] [Need: 00:09:25] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [174][000/391]   Time 0.319 (0.319)   Data 0.261 (0.261)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:02:44]
  Epoch: [174][200/391]   Time 0.049 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:02:53]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.410 Prec@5 99.710 Error@1 7.590
  **Test** Prec@1 92.410 Prec@5 99.710 Error@1 7.590

==>>[2018-11-14 12:03:05] [Epoch=175/200] [Need: 00:09:04] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [175][000/391]   Time 0.328 (0.328)   Data 0.271 (0.271)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:03:06]
  Epoch: [175][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0027 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:03:15]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.280 Prec@5 99.630 Error@1 7.720
  **Test** Prec@1 92.280 Prec@5 99.630 Error@1 7.720

==>>[2018-11-14 12:03:27] [Epoch=176/200] [Need: 00:08:42] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [176][000/391]   Time 0.313 (0.313)   Data 0.265 (0.265)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:03:28]
  Epoch: [176][200/391]   Time 0.043 (0.047)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:03:37]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.320 Prec@5 99.690 Error@1 7.680
  **Test** Prec@1 92.320 Prec@5 99.690 Error@1 7.680

==>>[2018-11-14 12:03:49] [Epoch=177/200] [Need: 00:08:20] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [177][000/391]   Time 0.321 (0.321)   Data 0.261 (0.261)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:03:50]
  Epoch: [177][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0025 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:03:59]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.430 Prec@5 99.700 Error@1 7.570
  **Test** Prec@1 92.430 Prec@5 99.700 Error@1 7.570

==>>[2018-11-14 12:04:11] [Epoch=178/200] [Need: 00:07:58] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [178][000/391]   Time 0.309 (0.309)   Data 0.254 (0.254)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:04:11]
  Epoch: [178][200/391]   Time 0.042 (0.047)   Data 0.000 (0.002)   Loss 0.0016 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:04:21]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.470 Prec@5 99.720 Error@1 7.530
  **Test** Prec@1 92.470 Prec@5 99.720 Error@1 7.530

==>>[2018-11-14 12:04:33] [Epoch=179/200] [Need: 00:07:37] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [179][000/391]   Time 0.322 (0.322)   Data 0.268 (0.268)   Loss 0.0025 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:04:33]
  Epoch: [179][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0022 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:04:42]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.390 Prec@5 99.700 Error@1 7.610
  **Test** Prec@1 92.390 Prec@5 99.700 Error@1 7.610

==>>[2018-11-14 12:04:54] [Epoch=180/200] [Need: 00:07:15] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [180][000/391]   Time 0.373 (0.373)   Data 0.299 (0.299)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:04:55]
  Epoch: [180][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:05:04]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.350 Prec@5 99.660 Error@1 7.650
  **Test** Prec@1 92.350 Prec@5 99.660 Error@1 7.650

==>>[2018-11-14 12:05:16] [Epoch=181/200] [Need: 00:06:53] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [181][000/391]   Time 0.324 (0.324)   Data 0.270 (0.270)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:05:17]
  Epoch: [181][200/391]   Time 0.042 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:05:25]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.370 Prec@5 99.710 Error@1 7.630
  **Test** Prec@1 92.370 Prec@5 99.710 Error@1 7.630

==>>[2018-11-14 12:05:38] [Epoch=182/200] [Need: 00:06:31] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [182][000/391]   Time 0.327 (0.327)   Data 0.272 (0.272)   Loss 0.0021 (0.0021)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:05:38]
  Epoch: [182][200/391]   Time 0.049 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:05:47]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.410 Prec@5 99.730 Error@1 7.590
  **Test** Prec@1 92.410 Prec@5 99.730 Error@1 7.590

==>>[2018-11-14 12:06:00] [Epoch=183/200] [Need: 00:06:09] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [183][000/391]   Time 0.324 (0.324)   Data 0.266 (0.266)   Loss 0.0024 (0.0024)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:06:00]
  Epoch: [183][200/391]   Time 0.047 (0.047)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:06:09]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.490 Prec@5 99.700 Error@1 7.510
  **Test** Prec@1 92.490 Prec@5 99.700 Error@1 7.510

==>>[2018-11-14 12:06:22] [Epoch=184/200] [Need: 00:05:48] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [184][000/391]   Time 0.302 (0.302)   Data 0.245 (0.245)   Loss 0.0026 (0.0026)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:06:22]
  Epoch: [184][200/391]   Time 0.048 (0.047)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:06:31]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.350 Prec@5 99.700 Error@1 7.650
  **Test** Prec@1 92.350 Prec@5 99.700 Error@1 7.650

==>>[2018-11-14 12:06:44] [Epoch=185/200] [Need: 00:05:26] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [185][000/391]   Time 0.319 (0.319)   Data 0.261 (0.261)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:06:44]
  Epoch: [185][200/391]   Time 0.041 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:06:53]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.380 Prec@5 99.720 Error@1 7.620
  **Test** Prec@1 92.380 Prec@5 99.720 Error@1 7.620

==>>[2018-11-14 12:07:06] [Epoch=186/200] [Need: 00:05:04] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [186][000/391]   Time 0.339 (0.339)   Data 0.284 (0.284)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:07:06]
  Epoch: [186][200/391]   Time 0.052 (0.047)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:07:15]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.390 Prec@5 99.700 Error@1 7.610
  **Test** Prec@1 92.390 Prec@5 99.700 Error@1 7.610

==>>[2018-11-14 12:07:28] [Epoch=187/200] [Need: 00:04:42] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [187][000/391]   Time 0.300 (0.300)   Data 0.247 (0.247)   Loss 0.0021 (0.0021)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:07:28]
  Epoch: [187][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0025 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:07:37]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.430 Prec@5 99.740 Error@1 7.570
  **Test** Prec@1 92.430 Prec@5 99.740 Error@1 7.570

==>>[2018-11-14 12:07:49] [Epoch=188/200] [Need: 00:04:21] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [188][000/391]   Time 0.338 (0.338)   Data 0.282 (0.282)   Loss 0.0025 (0.0025)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:07:50]
  Epoch: [188][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:07:59]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.430 Prec@5 99.710 Error@1 7.570
  **Test** Prec@1 92.430 Prec@5 99.710 Error@1 7.570

==>>[2018-11-14 12:08:11] [Epoch=189/200] [Need: 00:03:59] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [189][000/391]   Time 0.328 (0.328)   Data 0.271 (0.271)   Loss 0.0022 (0.0022)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:08:11]
  Epoch: [189][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:08:20]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.330 Prec@5 99.680 Error@1 7.670
  **Test** Prec@1 92.330 Prec@5 99.680 Error@1 7.670

==>>[2018-11-14 12:08:33] [Epoch=190/200] [Need: 00:03:37] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [190][000/391]   Time 0.302 (0.302)   Data 0.248 (0.248)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:08:33]
  Epoch: [190][200/391]   Time 0.049 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:08:42]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.500 Prec@5 99.740 Error@1 7.500
  **Test** Prec@1 92.500 Prec@5 99.740 Error@1 7.500

==>>[2018-11-14 12:08:54] [Epoch=191/200] [Need: 00:03:15] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [191][000/391]   Time 0.299 (0.299)   Data 0.243 (0.243)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:08:55]
  Epoch: [191][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0018 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:09:04]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.400 Prec@5 99.720 Error@1 7.600
  **Test** Prec@1 92.400 Prec@5 99.720 Error@1 7.600

==>>[2018-11-14 12:09:16] [Epoch=192/200] [Need: 00:02:54] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [192][000/391]   Time 0.330 (0.330)   Data 0.267 (0.267)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:09:16]
  Epoch: [192][200/391]   Time 0.047 (0.046)   Data 0.000 (0.002)   Loss 0.0022 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:09:25]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.470 Prec@5 99.730 Error@1 7.530
  **Test** Prec@1 92.470 Prec@5 99.730 Error@1 7.530

==>>[2018-11-14 12:09:38] [Epoch=193/200] [Need: 00:02:32] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [193][000/391]   Time 0.310 (0.310)   Data 0.254 (0.254)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:09:38]
  Epoch: [193][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0020 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:09:47]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.500 Prec@5 99.720 Error@1 7.500
  **Test** Prec@1 92.500 Prec@5 99.720 Error@1 7.500

==>>[2018-11-14 12:09:59] [Epoch=194/200] [Need: 00:02:10] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [194][000/391]   Time 0.308 (0.308)   Data 0.255 (0.255)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:10:00]
  Epoch: [194][200/391]   Time 0.043 (0.046)   Data 0.000 (0.002)   Loss 0.0023 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:10:09]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.270 Prec@5 99.700 Error@1 7.730
  **Test** Prec@1 92.270 Prec@5 99.700 Error@1 7.730

==>>[2018-11-14 12:10:21] [Epoch=195/200] [Need: 00:01:48] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [195][000/391]   Time 0.311 (0.311)   Data 0.256 (0.256)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:10:22]
  Epoch: [195][200/391]   Time 0.046 (0.047)   Data 0.000 (0.002)   Loss 0.0023 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:10:31]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.440 Prec@5 99.740 Error@1 7.560
  **Test** Prec@1 92.440 Prec@5 99.740 Error@1 7.560

==>>[2018-11-14 12:10:43] [Epoch=196/200] [Need: 00:01:27] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [196][000/391]   Time 0.321 (0.321)   Data 0.263 (0.263)   Loss 0.0017 (0.0017)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:10:43]
  Epoch: [196][200/391]   Time 0.046 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:10:52]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.450 Prec@5 99.740 Error@1 7.550
  **Test** Prec@1 92.450 Prec@5 99.740 Error@1 7.550

==>>[2018-11-14 12:11:05] [Epoch=197/200] [Need: 00:01:05] [learning_rate=0.0008] [Best : Accuracy=92.52, Error=7.48]
  Epoch: [197][000/391]   Time 0.312 (0.312)   Data 0.249 (0.249)   Loss 0.0015 (0.0015)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:11:05]
  Epoch: [197][200/391]   Time 0.045 (0.046)   Data 0.000 (0.002)   Loss 0.0016 (0.0020)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:11:14]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.540 Prec@5 99.720 Error@1 7.460
  **Test** Prec@1 92.540 Prec@5 99.720 Error@1 7.460

==>>[2018-11-14 12:11:27] [Epoch=198/200] [Need: 00:00:43] [learning_rate=0.0008] [Best : Accuracy=92.54, Error=7.46]
  Epoch: [198][000/391]   Time 0.320 (0.320)   Data 0.259 (0.259)   Loss 0.0016 (0.0016)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:11:27]
  Epoch: [198][200/391]   Time 0.047 (0.047)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:11:36]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.500 Prec@5 99.720 Error@1 7.500
  **Test** Prec@1 92.500 Prec@5 99.720 Error@1 7.500

==>>[2018-11-14 12:11:49] [Epoch=199/200] [Need: 00:00:21] [learning_rate=0.0008] [Best : Accuracy=92.54, Error=7.46]
  Epoch: [199][000/391]   Time 0.334 (0.334)   Data 0.280 (0.280)   Loss 0.0019 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:11:49]
  Epoch: [199][200/391]   Time 0.044 (0.046)   Data 0.000 (0.002)   Loss 0.0017 (0.0019)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2018-11-14 12:11:58]
  **Train** Prec@1 100.000 Prec@5 100.000 Error@1 0.000
  **Test** Prec@1 92.530 Prec@5 99.730 Error@1 7.470
  **Test** Prec@1 92.530 Prec@5 99.730 Error@1 7.470
